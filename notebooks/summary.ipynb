{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "q5sjerXIZAr5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3290,
     "status": "ok",
     "timestamp": 1701171878050,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "q5sjerXIZAr5",
    "outputId": "cf127ac1-3a8b-4da8-a6ff-81e312e3fac2"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (625918773.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    cd drive/MyDrive/PubPulse\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# if run in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "cd drive/MyDrive/PubPulse\n",
    "pwd\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/PubPulse/lib')\n",
    "sys.path.append('/content/drive/MyDrive/PubPulse/data')\n",
    "sys.path.append('/content/drive/MyDrive/PubPulse/services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d467414",
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1701171878733,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "9d467414"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mezcla/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../lib\")\n",
    "sys.path.append(\"../data\")\n",
    "sys.path.append(\"../services\")\n",
    "sys.path.append(\"../models\")\n",
    "import importlib\n",
    "import preproc as pre\n",
    "importlib.reload(pre)\n",
    "import search_service as search\n",
    "importlib.reload(search)\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be4c22-4919-49e9-9629-128e7a59eac9",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d09951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre.make_dataset_from_txt('../data/extracted-text/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a295416",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1701171878736,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "0a295416",
    "outputId": "ae602b2a-d68f-436f-b7c0-405f9502bd6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fI-fcT-gac4H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1701171878738,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "fI-fcT-gac4H",
    "outputId": "2fd29550-f1a6-4ad8-ffd9-ac4396816c0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>input</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR CONTROLLED</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>SYSTEMIC RNA DELIVERY</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                        title  \\\n",
       "0         ABSTRACT                                     ABSTRACT   \n",
       "1  TITLE_PARAGRAPH                                 INTRODUCTION   \n",
       "2  TITLE_PARAGRAPH         DRUG DELIVERY SYSTEMS FOR CONTROLLED   \n",
       "3  TITLE_PARAGRAPH                        SYSTEMIC RNA DELIVERY   \n",
       "4  TITLE_PARAGRAPH  DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY   \n",
       "\n",
       "                                               input  \\\n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...   \n",
       "1  Medicine relies on the use of pharmacologicall...   \n",
       "2  RELEASE One important class of DDS is controll...   \n",
       "3  RNAs can manipulate gene expression through se...   \n",
       "4  One potential limitation to systemic administr...   \n",
       "\n",
       "                               data_source  \n",
       "0  Emerging Frontiers in Drug Delivery.txt  \n",
       "1  Emerging Frontiers in Drug Delivery.txt  \n",
       "2  Emerging Frontiers in Drug Delivery.txt  \n",
       "3  Emerging Frontiers in Drug Delivery.txt  \n",
       "4  Emerging Frontiers in Drug Delivery.txt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f909b95f-5b9f-4c98-bfad-59bc8b5979be",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = ' '.join(dataset[(dataset.data_source == 'Emerging Frontiers in Drug Delivery.txt') &\\\n",
    "                                  (dataset.type != 'ABSTRACT')].input.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f39f783-18cd-4909-8a2f-4b81ffb76458",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pre.convert_pdf_to_txt('../data/raw-pdf/PMC8198544.pdf',\n",
    "                             'sample_txt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe88181-e13c-4267-a670-363e85334a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = dataset[(dataset.data_source == 'Emerging Frontiers in Drug Delivery.txt') &\\\n",
    "                                  (dataset.type != 'ABSTRACT')].input.tolist()\n",
    "paragraphs = [paragraph for paragraph in paragraphs if len(paragraph) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0645b431-0a19-412e-8abf-6ea13bcfe406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n",
      "964\n",
      "689\n",
      "2804\n",
      "748\n",
      "736\n",
      "2648\n",
      "251\n"
     ]
    }
   ],
   "source": [
    "for paragraph in paragraphs:\n",
    "    print(len(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fb4f5c-6acb-4c5d-a03d-3cc30938e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 1. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b970a42d-4037-4fe5-a446-f16b5f64cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_token_counts = [len(tokenizer.tokenize(text)) for text in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75236b8a-f7fe-4250-af62-c38b0d398336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre.plot_token_distribution(paragraphs_token_counts,\n",
    "                            \"Distribution of token per paragraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded816e-f130-4a87-b320-12f4fa9258b6",
   "metadata": {},
   "source": [
    "We must deal with the paragraphs that exceed the token limit of **512** the model `llama-2-7b.Q2_K.gguf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537d0061-e6ac-4acd-a331-2a9eeb3a3781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500,\n",
       " 500,\n",
       " 176,\n",
       " 500,\n",
       " 464,\n",
       " 500,\n",
       " 189,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 304,\n",
       " 500,\n",
       " 248,\n",
       " 500,\n",
       " 236,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 148,\n",
       " 251]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to split and filter paragraphs\n",
    "def split_and_filter_paragraphs(paragraphs, max_length=500, min_length=100):\n",
    "    split_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # Split paragraph into chunks of approximately max_length characters\n",
    "        for i in range(0, len(paragraph), max_length):\n",
    "            chunk = paragraph[i:i+max_length]\n",
    "            # Only add chunks that are at least min_length characters long\n",
    "            if len(chunk) >= min_length:\n",
    "                split_paragraphs.append(chunk)\n",
    "    return split_paragraphs\n",
    "\n",
    "# Splitting and filtering the paragraphs\n",
    "processed_paragraphs = split_and_filter_paragraphs(paragraphs)\n",
    "\n",
    "# Printing the lengths of the processed paragraphs\n",
    "processed_paragraphs_lengths = [len(p) for p in processed_paragraphs]\n",
    "processed_paragraphs_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f22f68-8f01-4130-9220-05438cf113a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_paragraphs = [paragraph for paragraph, count in zip(split_paragraphs, split_paragraphs_token_counts) if count >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde132be-99a5-4648-9962-6d760851c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for par in filtered_paragraphs:\n",
    "#     print(len(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74a32ef-3f83-4a4d-bdc1-0fc4e22900ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_paragraphs_token_counts = [len(tokenizer.tokenize(text)) for text in filtered_paragraphs]\n",
    "# split_paragraphs_token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f73e-4b3d-447c-9fe3-4c9ef16f4dd8",
   "metadata": {},
   "source": [
    "# Llama-2-7b.Q2_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf5770d-218a-474b-bc26-f4fda0e0db52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 73.56 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77a70b9a-9d83-48f3-a889-d39f1b90f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  11%|████▋                                     | 1/9 [01:42<13:41, 102.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure).\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      73.10 ms /   117 runs   (    0.62 ms per token,  1600.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64469.50 ms /   283 tokens (  227.81 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =   37758.36 ms /   116 runs   (  325.50 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =  102661.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  22%|█████████▌                                 | 2/9 [02:37<08:41, 74.55s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.66 ms /     1 runs   (    0.66 ms per token,  1508.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54860.92 ms /   224 tokens (  244.91 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   54866.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  33%|██████████████▎                            | 3/9 [03:24<06:11, 61.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /     1 runs   (    0.67 ms per token,  1494.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46869.93 ms /   193 tokens (  242.85 ms per token,     4.12 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   46873.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  44%|███████████████████                        | 4/9 [04:46<05:49, 69.94s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.66 ms /     1 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82226.65 ms /   342 tokens (  240.43 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   82230.01 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  56%|███████████████████████▉                   | 5/9 [06:06<04:54, 73.52s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79876.43 ms /   332 tokens (  240.59 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   79880.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  67%|████████████████████████████▋              | 6/9 [06:52<03:12, 64.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     1 runs   (    0.91 ms per token,  1101.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45816.23 ms /   185 tokens (  247.66 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   45821.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  78%|█████████████████████████████████▍         | 7/9 [08:00<02:11, 65.53s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      28.55 ms /    50 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52303.81 ms /   198 tokens (  264.16 ms per token,     3.79 tokens per second)\n",
      "llama_print_timings:        eval time =   15950.22 ms /    49 runs   (  325.51 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =   68463.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the adult population has prediabetic hyperglycemia (PG); most PG do not progress to diabetes, which is associated with significant morbidity and premature mortality. 1\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      60.25 ms /    86 runs   (    0.70 ms per token,  1427.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   86531.95 ms /   322 tokens (  268.73 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =   32954.18 ms /    85 runs   (  387.70 ms per token,     2.58 tokens per second)\n",
      "llama_print_timings:       total time =  119877.82 ms\n",
      "Summarizing paragraphs:  89%|██████████████████████████████████████▏    | 8/9 [10:00<01:22, 82.84s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future of medicine will increasingly rely on drugs that target underlying genetic pathways. 2. Emerging platforms for RNA-based therapy have the potential to treat disease at a genetic level. 3. Oral administration of biologic medicines requires advanced drug delivery systems in order to deliver therapeutics to their site of action while minimizing off-target effects.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =     112.55 ms /   150 runs   (    0.75 ms per token,  1332.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80824.88 ms /   274 tokens (  294.98 ms per token,     3.39 tokens per second)\n",
      "llama_print_timings:        eval time =   66824.69 ms /   149 runs   (  448.49 ms per token,     2.23 tokens per second)\n",
      "llama_print_timings:       total time =  148429.00 ms\n",
      "Summarizing paragraphs: 100%|███████████████████████████████████████████| 9/9 [12:29<00:00, 83.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharmacological and genetic approaches to treating chronic disease, including the development of gene therapy medications, require an improved understanding of the interaction between drug delivery vehicles and molecular targets in the body; 2) living systems can be re-engineered to work with the body, and not against, to treat disease using the outstanding delivery mechanisms of microvesicles, pathogens, and cells (e.g., selective targeting, prolonged circulation, and immune tolerance); 3) these delivery mechanisms require the development of improved materials for the safe delivery of gene editing technologies; and 4) advanced delivery methods will likely improve as we understand how biological path\n",
      "-----------\n",
      "elivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure).      of the adult population has prediabetic hyperglycemia (PG); most PG do not progress to diabetes, which is associated with significant morbidity and premature mortality. 1 future of medicine will increasingly rely on drugs that target underlying genetic pathways. 2. Emerging platforms for RNA-based therapy have the potential to treat disease at a genetic level. 3. Oral administration of biologic medicines requires advanced drug delivery systems in order to deliver therapeutics to their site of action while minimizing off-target effects. pharmacological and genetic approaches to treating chronic disease, including the development of gene therapy medications, require an improved understanding of the interaction between drug delivery vehicles and molecular targets in the body; 2) living systems can be re-engineered to work with the body, and not against, to treat disease using the outstanding delivery mechanisms of microvesicles, pathogens, and cells (e.g., selective targeting, prolonged circulation, and immune tolerance); 3) these delivery mechanisms require the development of improved materials for the safe delivery of gene editing technologies; and 4) advanced delivery methods will likely improve as we understand how biological path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(filtered_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Correctly format the prompt with the current paragraph\n",
    "    formatted_prompt = \"Q: Create a summary of this {}. Summary: \".format(paragraph)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,\n",
    "        stop=[\"Q:\", \"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1da7ac8-2615-4a1f-9bf3-e8f869540efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                   | 0/9 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  11%|████▊                                      | 1/9 [01:15<10:06, 75.80s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /     4 runs   (    0.73 ms per token,  1368.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74671.63 ms /   276 tokens (  270.55 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1104.65 ms /     3 runs   (  368.22 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:       total time =   75796.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  22%|█████████▌                                 | 2/9 [02:30<08:46, 75.28s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      15.94 ms /    24 runs   (    0.66 ms per token,  1506.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65224.73 ms /   220 tokens (  296.48 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:        eval time =    9572.27 ms /    23 runs   (  416.19 ms per token,     2.40 tokens per second)\n",
      "llama_print_timings:       total time =   74907.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  33%|██████████████▎                            | 3/9 [03:36<07:06, 71.04s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      21.12 ms /    32 runs   (    0.66 ms per token,  1515.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54610.65 ms /   189 tokens (  288.95 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:        eval time =   11238.20 ms /    31 runs   (  362.52 ms per token,     2.76 tokens per second)\n",
      "llama_print_timings:       total time =   65986.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  44%|███████████████████                        | 4/9 [05:43<07:45, 93.19s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      48.77 ms /    76 runs   (    0.64 ms per token,  1558.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   99298.29 ms /   338 tokens (  293.78 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:        eval time =   27508.22 ms /    75 runs   (  366.78 ms per token,     2.73 tokens per second)\n",
      "llama_print_timings:       total time =  127140.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  56%|███████████████████████▉                   | 5/9 [07:18<06:14, 93.62s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1519.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   94007.37 ms /   328 tokens (  286.61 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:        eval time =     378.44 ms /     1 runs   (  378.44 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =   94394.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      37.44 ms /    52 runs   (    0.72 ms per token,  1388.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51814.44 ms /   181 tokens (  286.27 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:        eval time =   19349.07 ms /    51 runs   (  379.39 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =   71401.06 ms\n",
      "Summarizing paragraphs:  67%|████████████████████████████▋              | 6/9 [08:29<04:18, 86.07s/it]Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  78%|█████████████████████████████████▍         | 7/9 [10:00<02:55, 87.67s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      54.30 ms /    68 runs   (    0.80 ms per token,  1252.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59581.95 ms /   194 tokens (  307.12 ms per token,     3.26 tokens per second)\n",
      "llama_print_timings:        eval time =   31006.99 ms /    67 runs   (  462.79 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time =   90968.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  89%|██████████████████████████████████████▏    | 8/9 [11:50<01:34, 94.68s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      17.36 ms /    25 runs   (    0.69 ms per token,  1440.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101044.90 ms /   318 tokens (  317.75 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:        eval time =    8513.52 ms /    24 runs   (  354.73 ms per token,     2.82 tokens per second)\n",
      "llama_print_timings:       total time =  109671.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs: 100%|███████████████████████████████████████████| 9/9 [13:33<00:00, 97.43s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      35.72 ms /    41 runs   (    0.87 ms per token,  1147.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82434.99 ms /   270 tokens (  305.31 ms per token,     3.28 tokens per second)\n",
      "llama_print_timings:        eval time =   20775.58 ms /    40 runs   (  519.39 ms per token,     1.93 tokens per second)\n",
      "llama_print_timings:       total time =  103473.97 ms\n",
      "Summarizing paragraphs: 100%|███████████████████████████████████████████| 9/9 [13:33<00:00, 90.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1). These observations have major implications for the design and development of new drugs and the understanding of old ones. For example, a mutated form of the mRNA for beta-globin may be used to treat sickle-cell anemia. A potential strategy for improving these devices is to coat them with biodegradable DDS that control release of drugs for up to several months. This strategy has been used with some success in clinical trials and may offer significant advantages over stents alone, especially when the drug being released is a thrombolytic or antirestenosis agent. Thus, to prolong the residence time and enhance bioavailability of a drug, extended-release technology can be applied. This is an important strategy for biotherapeutics that are poorly absorbed or rapidly cleared from the bloodstream. 208 However, the harvesting, purification, and administration of these microvesicles are technically challenging for clinical applications. 209 In particular, controlling the size range and specificity of the microvesicle cargo is difficult to achieve in an efficient manner (Figure 15). The combination of advanced drug delivery technologies with molecular biology will be essential to treat disease at the root cause. In particular, polymers, aptamers, peptides, and nanofibers have all been shown to efficiently enter cells when combined with genetic editing reagents (97).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(filtered_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c6b6ed-ed69-4dc3-9a71-3b467e6244b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  750.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 123.75 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:   0%|                                                   | 0/8 [00:00<?, ?it/s]\n",
      "Summarizing paragraphs:  12%|█████▎                                    | 1/8 [02:00<14:02, 120.42s/it]llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =      30.66 ms /    44 runs   (    0.70 ms per token,  1435.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101522.86 ms /   277 tokens (  366.51 ms per token,     2.73 tokens per second)\n",
      "llama_print_timings:        eval time =   18678.72 ms /    43 runs   (  434.39 ms per token,     2.30 tokens per second)\n",
      "llama_print_timings:       total time =  120414.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  25%|██████████▊                                | 2/8 [03:14<09:18, 93.13s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       7.94 ms /    12 runs   (    0.66 ms per token,  1512.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70068.22 ms /   220 tokens (  318.49 ms per token,     3.14 tokens per second)\n",
      "llama_print_timings:        eval time =    3893.37 ms /    11 runs   (  353.94 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   74020.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  38%|████████████████▏                          | 3/8 [04:15<06:31, 78.34s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1365.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60383.97 ms /   189 tokens (  319.49 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:        eval time =     354.95 ms /     1 runs   (  354.95 ms per token,     2.82 tokens per second)\n",
      "llama_print_timings:       total time =   60748.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  62%|██████████████████████████▉                | 5/8 [05:05<02:24, 48.19s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1520.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49819.24 ms /   181 tokens (  275.24 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:        eval time =     319.29 ms /     1 runs   (  319.29 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:       total time =   50147.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  75%|████████████████████████████████▎          | 6/8 [06:14<01:48, 54.12s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =      34.54 ms /    51 runs   (    0.68 ms per token,  1476.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51667.20 ms /   194 tokens (  266.33 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:        eval time =   16911.76 ms /    50 runs   (  338.24 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =   68793.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs: 100%|███████████████████████████████████████████| 8/8 [06:39<00:00, 49.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1). In other words, DDS are custom-built to ensure that each drug is delivered in a therapeutic effective dose at a targeted site where it will exert its effect. The next section provides an overview of these concepts. For example, the islet beta-cells in type I diabetics have been replaced with insulin-producing cells derived from stem cells through microvesicle isolation and purification for autologous therapy (Figure The remaining authors have declared no interests with commercial supporters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    14 runs   (    0.61 ms per token,  1649.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20975.83 ms /    78 tokens (  268.92 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4588.45 ms /    13 runs   (  352.96 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   25622.28 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_m = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "           n_ctx=1500, n_gpu_layers=-1)\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_m(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "131305cd-0818-4cd9-90a5-e970c48caed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10062"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232e878b-bf9c-4ef9-a819-2cb53edca5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  250.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 71.91 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_l = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "              n_ctx=500, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eabb1bab-fa93-4989-9ad3-91ea81a5abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 1. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "tokens_combined_text = tokenizer.tokenize(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11505b63-a391-479e-8f2a-c409ca29a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   4%|█▊                                        | 1/24 [00:31<12:00, 31.33s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     1 runs   (    0.65 ms per token,  1529.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31312.92 ms /   134 tokens (  233.68 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   31318.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:   8%|███▌                                      | 2/24 [01:23<15:55, 43.41s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      63.01 ms /    99 runs   (    0.64 ms per token,  1571.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23624.82 ms /   109 tokens (  216.74 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:        eval time =   27897.62 ms /    98 runs   (  284.67 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   51865.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  12%|█████▎                                    | 3/24 [01:49<12:30, 35.72s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      40.55 ms /    65 runs   (    0.62 ms per token,  1603.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8376.86 ms /    38 tokens (  220.44 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:        eval time =   17966.03 ms /    64 runs   (  280.72 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =   26567.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  17%|███████                                   | 4/24 [02:27<12:07, 36.39s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      22.95 ms /    37 runs   (    0.62 ms per token,  1612.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27033.97 ms /   123 tokens (  219.79 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:        eval time =   10242.12 ms /    36 runs   (  284.50 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   37402.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  21%|████████▊                                 | 5/24 [03:02<11:22, 35.91s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      28.28 ms /    46 runs   (    0.61 ms per token,  1626.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22069.78 ms /   100 tokens (  220.70 ms per token,     4.53 tokens per second)\n",
      "llama_print_timings:        eval time =   12843.51 ms /    45 runs   (  285.41 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =   35070.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  25%|██████████▌                               | 6/24 [04:17<14:46, 49.27s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      90.45 ms /   150 runs   (    0.60 ms per token,  1658.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31742.35 ms /   139 tokens (  228.36 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:        eval time =   42915.27 ms /   149 runs   (  288.02 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   75206.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  29%|████████████▎                             | 7/24 [04:54<12:48, 45.23s/it]llama_print_timings:      sample time =      57.47 ms /    88 runs   (    0.65 ms per token,  1531.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11775.20 ms /    52 tokens (  226.45 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =   24810.96 ms /    87 runs   (  285.18 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   36904.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  33%|██████████████                            | 8/24 [05:29<11:11, 41.98s/it]llama_print_timings:      sample time =      18.69 ms /    32 runs   (    0.58 ms per token,  1711.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25983.07 ms /   114 tokens (  227.92 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =    8930.64 ms /    31 runs   (  288.09 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   35024.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  38%|███████████████▊                          | 9/24 [05:59<09:33, 38.25s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.64 ms /     1 runs   (    0.64 ms per token,  1557.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30047.05 ms /   132 tokens (  227.63 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   30050.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  42%|█████████████████                        | 10/24 [06:38<08:58, 38.43s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      24.99 ms /    38 runs   (    0.66 ms per token,  1520.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27846.07 ms /   123 tokens (  226.39 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =   10846.97 ms /    37 runs   (  293.16 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =   38831.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  46%|██████████████████▊                      | 11/24 [07:14<08:09, 37.63s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      24.92 ms /    39 runs   (    0.64 ms per token,  1565.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24660.97 ms /   108 tokens (  228.34 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:        eval time =   10994.62 ms /    38 runs   (  289.33 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   35799.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  50%|████████████████████▌                    | 12/24 [08:11<08:43, 43.60s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      67.32 ms /   100 runs   (    0.67 ms per token,  1485.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27901.11 ms /   122 tokens (  228.70 ms per token,     4.37 tokens per second)\n",
      "llama_print_timings:        eval time =   28981.87 ms /    99 runs   (  292.75 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =   57263.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  54%|██████████████████████▏                  | 13/24 [08:29<06:33, 35.76s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.62 ms /     1 runs   (    0.62 ms per token,  1602.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17693.15 ms /    76 tokens (  232.80 ms per token,     4.30 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17696.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  58%|███████████████████████▉                 | 14/24 [09:07<06:05, 36.58s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      26.61 ms /    40 runs   (    0.67 ms per token,  1503.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27059.91 ms /   117 tokens (  231.28 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:        eval time =   11262.42 ms /    39 runs   (  288.78 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   38468.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  62%|█████████████████████████▋               | 15/24 [09:42<05:24, 36.00s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      44.17 ms /    68 runs   (    0.65 ms per token,  1539.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15166.97 ms /    65 tokens (  233.34 ms per token,     4.29 tokens per second)\n",
      "llama_print_timings:        eval time =   19256.82 ms /    67 runs   (  287.42 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:       total time =   34671.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  67%|███████████████████████████▎             | 16/24 [10:39<05:39, 42.43s/it]llama_print_timings:      sample time =      56.43 ms /    87 runs   (    0.65 ms per token,  1541.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31470.75 ms /   135 tokens (  233.12 ms per token,     4.29 tokens per second)\n",
      "llama_print_timings:        eval time =   25538.44 ms /    86 runs   (  296.96 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:       total time =   57336.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  71%|█████████████████████████████            | 17/24 [11:37<05:30, 47.22s/it]llama_print_timings:      sample time =      99.48 ms /   150 runs   (    0.66 ms per token,  1507.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14146.97 ms /    60 tokens (  235.78 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =   43633.49 ms /   149 runs   (  292.84 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =   58352.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  75%|██████████████████████████████▊          | 18/24 [12:13<04:22, 43.68s/it]llama_print_timings:      sample time =      21.95 ms /    34 runs   (    0.65 ms per token,  1549.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25685.03 ms /   108 tokens (  237.82 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9630.66 ms /    33 runs   (  291.84 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =   35438.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  79%|████████████████████████████████▍        | 19/24 [12:58<03:40, 44.18s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      47.72 ms /    72 runs   (    0.66 ms per token,  1508.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24328.25 ms /   103 tokens (  236.20 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:        eval time =   20748.74 ms /    71 runs   (  292.24 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =   45341.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  83%|██████████████████████████████████▏      | 20/24 [13:40<02:54, 43.52s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      33.21 ms /    51 runs   (    0.65 ms per token,  1535.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27108.05 ms /   115 tokens (  235.72 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =   14693.68 ms /    50 runs   (  293.87 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time =   41997.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  88%|███████████████████████████████████▉     | 21/24 [14:10<01:57, 39.31s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /     5 runs   (    0.65 ms per token,  1537.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28318.07 ms /   122 tokens (  232.12 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1154.87 ms /     4 runs   (  288.72 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   29491.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  92%|█████████████████████████████████████▌   | 22/24 [14:40<01:13, 36.75s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       9.32 ms /    15 runs   (    0.62 ms per token,  1609.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26627.08 ms /   113 tokens (  235.64 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =    4074.32 ms /    14 runs   (  291.02 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =   30755.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.59 ms /     1 runs   (    0.59 ms per token,  1700.68 tokens per second)\n",
      "Summarizing paragraphs:  96%|███████████████████████████████████████▎ | 23/24 [14:49<00:28, 28.24s/it]llama_print_timings: prompt eval time =    8387.58 ms /    35 tokens (  239.65 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    8391.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs: 100%|█████████████████████████████████████████| 24/24 [15:16<00:00, 28.05s/it]llama_print_timings:      sample time =      18.92 ms /    31 runs   (    0.61 ms per token,  1638.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18390.38 ms /    78 tokens (  235.77 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9102.08 ms /    30 runs   (  303.40 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:       total time =   27612.19 ms\n",
      "Summarizing paragraphs: 100%|█████████████████████████████████████████| 24/24 [15:16<00:00, 38.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue-specific time. In addition, many approaches have been applied to limit the release of drugs to a particular environment or location. These strategies include the use of biodegradable polymers and liposomes that are able to degrade in situ at desired rates. The chemical strategy can also be used to control the rate of drug release in vivo by altering physicochemical properties, such as pH, osmolality, and viscosity. 2. савез 6.4). DDS can be classified by their release mechanism, which involves either passive diffusion into the bloodstream or active transport across a biological membrane, such as a cellular wall, using transporters that are specific to a particular molecule (13). ues to design controlled release systems. These cues include the shape of the drug, its physicochemical properties, and the release rate required by the targeted application. For example, oral drug delivery systems may interact with enzymes in the GI tract before reaching their target organ, which is important for understanding and predicting the pharmacokinetics of many drugs. untion, and infection [52–54]. To target RNAs for degradation in this tissue, a library of siRNA was first designed around an Fc-fragment derived from the antibody Fab region (Fig. 3a) [55] that targets specific serum albumin isotypes overexpressed in liver cirrhosis and hepatocellular carcinoma . A liposomal delivery system was used to transfect the cells with siRNA targeted to these isotypes (Fig. 3b) [57]. When transfected into hepatocytes, the albumin siRNAs caused reduced production of sg Moreover, mRNA technology is a universal platform that allows for the expression of any target protein, even if its nucleotide sequence has not yet been fully characterized. In this chapter we describe the different mRNA technologies used to express human proteins in various biological settings. We discuss the challenges associated with the translation and delivery of these therapeutics and summarize the current state of the field. -d carbonate particles in rats with ovarian cancer has resulted in significantly higher therapeutic concentrations than those achieved systemically. ection, In Situ Forming (ISF) is an emerging approach that combines hydrogels and drug delivery systems to form drug-loaded depots within tissue. ps. As the temperature is decreased from above body temperature, thermally responsive hydrogels may rapidly undergo a transition from a fluid phase to a solid phase (Figure ability, we are designing a class of polymers that can be injected into any tissue of choice. These polymers will have high molecular weight (MW) to maintain rigidity and prevent precipitation during normal operation. The MW also determines the rate at which the polymer dissolves, which is controlled by the solubility parameter S(r), where r is the distance between two atoms in a chain of repeating monomers (Figure 1). ­administrable biologic (NDS-001) that is a monoclonal antibody directed against interferon-α (IFN-α). The pharmacokinetics of semaglutide are linear over a dose range of 1.5-15 mg once-weekly and 7.4-74 mg once-daily for 2 wks, with mean half life of approximately 60 min [9]. rosstalk among different cell types, to regulate signaling pathways, to participate in innate immunity, and to deliver cargo (e.g., proteins, lipids, nucleic acids) to various organs and tissues. 208–210 These natural drug carriers are of interest as vehicles for delivery of therapeutics to specific locations within the body. Several different types of human tissue are used in such procedures: hematopoietic stem cells (HSC) for blood formation; mesenchymal stem cells (MSC) for bone, cartilage and marrow repair; and endothelial or other precursor cells to facilitate vascularization. In addition, patient-derived tumor cells can be used for genetic modification by gene transfer or transgenic construction in the context of gene therapy. The development of new methods and reagents has made this field more attractive than ever, but major technical issues remain to be addressed, including (1) the isolation of functional HSCs from bone marrow and other t ances in polymer chemistry, physical properties, drug formulation and processing methods, and delivery technologies to address the ever-increasing needs of patients. ended to receive little attention from scientists working in the field of pharmaceutical engineering and formulation science. We hope, by outlining what is possible today and highlighting promising techniques on the horizon, that you will begin to consider how you might contribute your skills, experience and expertise toward this growing field of drug delivery research. -risk development and allow for long-acting treatments with decreased side effects. These are just a few examples of how nanotechnology is revolutionizing medicine, and its impact on drug delivery will be felt across the globe. 2542 the nuclear envelope. The article is accompanied by a news release. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # # Skip paragraphs that are too long\n",
    "    # if len(tokens) > 512:\n",
    "    #     continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_l(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d5e2df4-71c6-448c-826e-16f9b7ad8b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'issue-specific time. In addition, many approaches have been applied to limit the release of drugs to a particular environment or location. These strategies include the use of biodegradable polymers and liposomes that are able to degrade in situ at desired rates. The chemical strategy can also be used to control the rate of drug release in vivo by altering physicochemical properties, such as pH, osmolality, and viscosity. 2. савез 6.4). DDS can be classified by their release mechanism, which involves either passive diffusion into the bloodstream or active transport across a biological membrane, such as a cellular wall, using transporters that are specific to a particular molecule (13). ues to design controlled release systems. These cues include the shape of the drug, its physicochemical properties, and the release rate required by the targeted application. For example, oral drug delivery systems may interact with enzymes in the GI tract before reaching their target organ, which is important for understanding and predicting the pharmacokinetics of many drugs. untion, and infection [52–54]. To target RNAs for degradation in this tissue, a library of siRNA was first designed around an Fc-fragment derived from the antibody Fab region (Fig. 3a) [55] that targets specific serum albumin isotypes overexpressed in liver cirrhosis and hepatocellular carcinoma . A liposomal delivery system was used to transfect the cells with siRNA targeted to these isotypes (Fig. 3b) [57]. When transfected into hepatocytes, the albumin siRNAs caused reduced production of sg Moreover, mRNA technology is a universal platform that allows for the expression of any target protein, even if its nucleotide sequence has not yet been fully characterized. In this chapter we describe the different mRNA technologies used to express human proteins in various biological settings. We discuss the challenges associated with the translation and delivery of these therapeutics and summarize the current state of the field. -d carbonate particles in rats with ovarian cancer has resulted in significantly higher therapeutic concentrations than those achieved systemically. ection, In Situ Forming (ISF) is an emerging approach that combines hydrogels and drug delivery systems to form drug-loaded depots within tissue. ps. As the temperature is decreased from above body temperature, thermally responsive hydrogels may rapidly undergo a transition from a fluid phase to a solid phase (Figure ability, we are designing a class of polymers that can be injected into any tissue of choice. These polymers will have high molecular weight (MW) to maintain rigidity and prevent precipitation during normal operation. The MW also determines the rate at which the polymer dissolves, which is controlled by the solubility parameter S(r), where r is the distance between two atoms in a chain of repeating monomers (Figure 1). \\xadadministrable biologic (NDS-001) that is a monoclonal antibody directed against interferon-α (IFN-α). The pharmacokinetics of semaglutide are linear over a dose range of 1.5-15 mg once-weekly and 7.4-74 mg once-daily for 2 wks, with mean half life of approximately 60 min [9]. rosstalk among different cell types, to regulate signaling pathways, to participate in innate immunity, and to deliver cargo (e.g., proteins, lipids, nucleic acids) to various organs and tissues. 208–210 These natural drug carriers are of interest as vehicles for delivery of therapeutics to specific locations within the body. Several different types of human tissue are used in such procedures: hematopoietic stem cells (HSC) for blood formation; mesenchymal stem cells (MSC) for bone, cartilage and marrow repair; and endothelial or other precursor cells to facilitate vascularization. In addition, patient-derived tumor cells can be used for genetic modification by gene transfer or transgenic construction in the context of gene therapy. The development of new methods and reagents has made this field more attractive than ever, but major technical issues remain to be addressed, including (1) the isolation of functional HSCs from bone marrow and other t ances in polymer chemistry, physical properties, drug formulation and processing methods, and delivery technologies to address the ever-increasing needs of patients. ended to receive little attention from scientists working in the field of pharmaceutical engineering and formulation science. We hope, by outlining what is possible today and highlighting promising techniques on the horizon, that you will begin to consider how you might contribute your skills, experience and expertise toward this growing field of drug delivery research. -risk development and allow for long-acting treatments with decreased side effects. These are just a few examples of how nanotechnology is revolutionizing medicine, and its impact on drug delivery will be felt across the globe. 2542 the nuclear envelope. The article is accompanied by a news release. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb5c38-4e85-4b1b-b0da-824be67bdb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 73.56 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:   0%|                                                   | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\")\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Format the prompt with the individual paragraph\n",
    "    formatted_prompt = \"\"\"Summarize the following text, include only full sentences, \n",
    "                           don't include stuff like Table, Figure, \n",
    "                           create fluent text that can be merged with other snippets: {}\\nSummary:\"\"\".format(paragraph)\n",
    "\n",
    "    # formatted_prompt = \"\"\"Summarize the following text, include only full sentences, \n",
    "    #                        don't include stuff like Table, Figure, \n",
    "    #                        create fluent text that can be merged with other snippets: {}\\nSummary:\"\"\".format(paragraph)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9598600-2129-4d12-be94-74808014d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xt above is summarized by the following sentence fragments:  ent routes of administration, such as injection and oral ingestion, have different challenges when administering drugs to patients. Nordisk has developed a new drug delivery system (DDS) that overcomes many obstacles associated with delivering biologic therapeutics. ically based drug delivery systems (DDS) are gaining interest as a means for the targeted administration of therapeutics, including vaccines, to treat disease states,  such as diabetes mellitus and heart failure (HF).  DDS have been shown to be effective in eliciting an immune response after injection into the peritoneum.  These approaches are generally safer than conventional therapies with fewer side effects or complications when compared with traditional therapies, such as chemotherapy and insulin injections for diabetes mellitus patients. '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "340c9047-9cf6-4e74-8f24-3b101bc3b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                   | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (2450) exceed context window of 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(paragraph))\n\u001b[1;32m      6\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSummarize the following text, include only full sentences, \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                       don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt include stuff like Table, Figure, \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m                       create fluent text that can be merged with other snippets: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(paragraphs)\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m summary_index \u001b[38;5;241m=\u001b[39m generated_text\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1996\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1934\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1959\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1929\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1929\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1415\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m-> 1415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1417\u001b[0m     )\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (2450) exceed context window of 512"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    print(len(paragraph))\n",
    "    formatted_prompt = \"\"\"Summarize the following text, include only full sentences, \n",
    "                           don't include stuff like Table, Figure, \n",
    "                           create fluent text that can be merged with other snippets: {}\\nSummary:\"\"\".format(paragraphs)\n",
    "    \n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69430c95-2b66-4d76-b354-e5a04ba612b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7caa5-002e-427a-997c-2f33a67faf98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb24312-bd40-4279-93a9-9cebacc46da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1). Drug delivery technologies can be classified into three primary categories depending on their physical form during administration: solid, liquid, or gas. Solid-form systems include implants, depots, particles, and capsules. Liquid-form devices contain a controlled-release suspension (CRS) or liposomes. Finally, gas-form technologies rely on propulsion mechanisms such as compressed gases (e.g., CO2), pressurized fluids, or surface tension–based techniques to transport and deliver drugs in vivo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      77.74 ms /   125 runs   (    0.62 ms per token,  1607.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   38708.79 ms /   125 runs   (  309.67 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   39198.73 ms\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with the text to summarize\n",
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "# Call the language model with the formatted prompt\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=200,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "# Get the generated text from the output\n",
    "generated_text = output['choices'][0]['text']\n",
    "\n",
    "# Find the position of \"Summary:\" in the text\n",
    "summary_index = generated_text.find(\"Summary:\")\n",
    "\n",
    "# Extract everything after \"Summary:\"\n",
    "summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "\n",
    "# Print the extracted summary\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7260b5b2-907c-48b3-95ae-880ebc09b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize the following text: Medicine relies on the use of pharmacologically active agents (therapeutics or drugs) to manage or reverse the course of disease. The current global pharmaceutical market is valued at $980 billion annually, and, in the U.S., nearly 50% of the population has used at least one prescription medication in the past 30 days. \\nIn the ideal case, drugs would be applied in vivo at exactly the therapeutic concentration and would precisely target cells that cause disease. However, drug delivery is not easily controlled. Drug release rates, cell-and tissue-specific targeting, and drug stability are difficult to predict. To address these limitations, drug delivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure\\nSummary: In this chapter we will discuss the current state-of-the-art technologies that are used to enhance therapeutic efficacy. These novel approaches have been developed to address issues related to poor bioavailability, inefficient penetration across biological barriers, and delivery of therapeutics at targeted sites within the body. We will briefly review several strategies for delivering agents to cells and tissues that are currently being tested clinically or are under development.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92e87ead-5552-49e6-9c20-728f69e934b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-3464f2b1-a673-4d55-9a81-9382fda885bb', 'object': 'text_completion', 'created': 1701342357, 'model': '../models/llama-2-7b.Q2_K.gguf', 'choices': [{'text': 'Summarize the following text: Medicine relies on the use of pharmacologically active agents (therapeutics or drugs) to manage or reverse the course of disease. The current global pharmaceutical market is valued at $980 billion annually, and, in the U.S., nearly 50% of the population has used at least one prescription medication in the past 30 days. \\nIn the ideal case, drugs would be applied in vivo at exactly the therapeutic concentration and would precisely target cells that cause disease. However, drug delivery is not easily controlled. Drug release rates, cell-and tissue-specific targeting, and drug stability are difficult to predict. To address these limitations, drug delivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure\\nSummary: This paper introduces a new paradigm of science and medicine based on holistic human-centered technologies. We review recent developments in targeting cells by size, genetics, or biomarkers and then showcase emerging strategies for localizing therapeutic agents to diseased tissues via nanoscale and molecularly engineered drug delivery systems (DDS). We also discuss the unique properties of DDS that make them critical components in personalized medicine. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 281, 'completion_tokens': 105, 'total_tokens': 386}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      64.93 ms /   105 runs   (    0.62 ms per token,  1617.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   85886.35 ms /   280 tokens (  306.74 ms per token,     3.26 tokens per second)\n",
      "llama_print_timings:        eval time =   40193.95 ms /   104 runs   (  386.48 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =  126543.56 ms\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with the text to summarize\n",
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "# Call the language model with the formatted prompt\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=500,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4709ffa6-a852-49af-a03c-0721357639ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Drug delivery systems (DDS) are designed to control the release rate and specificity of pharmacologically active agents (drugs). DDS may be formulated as suspensions, gels, or particles. The targeted site for drug delivery is often defined by chemical attributes of the therapeutic agent itself. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      43.36 ms /    71 runs   (    0.61 ms per token,  1637.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   23284.97 ms /    71 runs   (  327.96 ms per token,     3.05 tokens per second)\n",
      "llama_print_timings:       total time =   23568.61 ms\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=200,  # Increased max_tokens\n",
    "    stop=[\"\\n\"],     # Adjusted stop token\n",
    "    echo=False       # Turn off echo\n",
    ")\n",
    "\n",
    "# Extract the generated text from the output\n",
    "generated_text = output['choices'][0]['text']\n",
    "\n",
    "# Print the generated text (which should be the summary)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf672b7-f555-4179-a684-ce383bae66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text_chunk):\n",
    "    # Defining the template to generate summary\n",
    "    template = \"\"\"\n",
    "    Write a concise summary of the text, return your responses with 10 lines that cover the key points of the text.\n",
    "    ```{text}```\n",
    "    SUMMARY:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, \n",
    "                            input_variables=[\"text\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=LLM)\n",
    "\n",
    "    summary = llm_chain.run(text_chunk)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149abdb3-6a8e-4df2-ba43-d14083935a73",
   "metadata": {},
   "source": [
    "# llama-2-7b-chat.ggmlv3.q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dedc3-0367-429e-92fe-2b1cc58a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your local model directory\n",
    "local_model_directory = \"../models/llama-2-7b-chat.ggmlv3.q8_0\"  # Adjust this path\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_directory)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4032cb-b67c-4a2e-b166-86095a0444e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline\n",
    "prompt = f\"Summarize this: {}\".format(combined_text)\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Summary:\\n\"\n",
    "pipe = pipeline(task=\"summarization\", # \"text-generation\" \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_length=200)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fecb9-7a53-4594-9a32-3f3920b4789a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be332b7e-07e1-4e99-a27c-e067fd09647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "685a7726-e99c-4ccf-a421-c6dd7385438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM = Llama(model_path=\"../models/llama-2-7b-chat.ggmlv3.q8_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e61e8a-67af-493b-be68-7d0be838a214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b4f7fb107f4024904207505e0d5d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7942eaf862247058bc01d221ed2e8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461b6107074841ec8a807e4048d64b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c53b92f4e4c4e0992daf677c254a2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"austinm2151/Llama2-7b-Summarizer\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"austinm2151/Llama2-7b-Summarizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a6b78-d651-4ff3-ba0c-e61e0b6c0ba7",
   "metadata": {},
   "source": [
    "# NousResearch/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb0ca7-b2c4-40c3-836a-429e6ab9ad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8119a36f661f4344ab8c9f87bc245771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa808954d6fe46b2bc58e65d291c937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384c1206c84b461e91d14f1be01aff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c15376e0ad45efab76608e7e82cb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddc70837d4242be94633f8c57a47711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b01ac-72b9-4dee-8ee5-b08f350b17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Run text generation pipeline\n",
    "prompt = f\"Summarize this: {combined_text}\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Summary:\\n\"\n",
    "\n",
    "pipe = pipeline(task=\"summarization\", # \"text-generation\" \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59b36b9-2e4a-448e-9648-5634c2fa35ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>input</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR CONTROLLED</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>SYSTEMIC RNA DELIVERY</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                        title  \\\n",
       "0         ABSTRACT                                     ABSTRACT   \n",
       "1  TITLE_PARAGRAPH                                 INTRODUCTION   \n",
       "2  TITLE_PARAGRAPH         DRUG DELIVERY SYSTEMS FOR CONTROLLED   \n",
       "3  TITLE_PARAGRAPH                        SYSTEMIC RNA DELIVERY   \n",
       "4  TITLE_PARAGRAPH  DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY   \n",
       "\n",
       "                                               input  \\\n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...   \n",
       "1  Medicine relies on the use of pharmacologicall...   \n",
       "2  RELEASE One important class of DDS is controll...   \n",
       "3  RNAs can manipulate gene expression through se...   \n",
       "4  One potential limitation to systemic administr...   \n",
       "\n",
       "                               data_source  \n",
       "0  Emerging Frontiers in Drug Delivery.txt  \n",
       "1  Emerging Frontiers in Drug Delivery.txt  \n",
       "2  Emerging Frontiers in Drug Delivery.txt  \n",
       "3  Emerging Frontiers in Drug Delivery.txt  \n",
       "4  Emerging Frontiers in Drug Delivery.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5396c49-f263-468c-b191-615e89558675",
   "metadata": {},
   "source": [
    "# Llama2 Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce5250a6-6949-459a-b5d6-b835ba06a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bd11e71-cf15-482b-85c0-8efc695c9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Path to your model directory\n",
    "# model_directory = \"../models/Llama2-7b-Summarizer\" \n",
    "\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# # Load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "# # Prepare your input text\n",
    "# input_text = combined_text\n",
    "# input_ids = tokenizer.encode(pdf, \n",
    "#                              return_tensors='pt')\n",
    "\n",
    "# # Set the desired maximum length for the generated text\n",
    "# max_length = 200\n",
    "\n",
    "# # Generate output\n",
    "# output = model.generate(input_ids, max_length=max_length)\n",
    "\n",
    "# # Decode the output to text\n",
    "# decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a89815b-6c48-4d1b-82ac-a75c1ddd63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your model directory\n",
    "model_directory = \"../models/Llama2-7b-Summarizer\" \n",
    "\n",
    "def summarize(input_text, model_directory):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "    \n",
    "    # Prepare your input text\n",
    "    input_text = input_text\n",
    "    input_ids = tokenizer.encode(pdf, \n",
    "                                 return_tensors='pt')\n",
    "    \n",
    "    # Set the desired maximum length for the generated text\n",
    "    max_length = 200\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(input_ids, max_length=max_length)\n",
    "    \n",
    "    # Decode the output to text\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "\n",
    "    return decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1f6d1a-897f-4dfc-9e62-9aa8ce6bca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_paragraphs_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97226c18-4a54-4352-beef-414675b04a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n",
      "964\n",
      "689\n",
      "2804\n",
      "748\n",
      "736\n",
      "2648\n",
      "251\n"
     ]
    }
   ],
   "source": [
    "for paragraph in paragraphs:\n",
    "    print(len(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0051327b-e38e-4749-92a2-c2e38b02a234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "176\n",
      "500\n",
      "464\n",
      "500\n",
      "189\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "304\n",
      "500\n",
      "248\n",
      "500\n",
      "236\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "148\n",
      "251\n"
     ]
    }
   ],
   "source": [
    "for paragraph in processed_paragraphs:\n",
    "    print(len(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d92cd0-b72c-4e46-9b68-70059c54f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                  | 0/24 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "model_directory = \"../models/Llama2-7b-Summarizer\" \n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    generated_text = summarize(paragraph, model_directory)\n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2200a-86a7-4640-bba1-33e7f12ac001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # # Skip paragraphs that are too long\n",
    "    # if len(tokens) > 512:\n",
    "    #     continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_l(\n",
    "        formatted_prompt,\n",
    "        max_tokens=100,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aef14f6c9a6412eaacec74534af65a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_974883c2255b4908a8b66646af6e2067",
       "IPY_MODEL_fc9d9a5ba2844a2e9d9040afc092f598",
       "IPY_MODEL_49bc2f1c67cf419581d1082f34a02fb9"
      ],
      "layout": "IPY_MODEL_c3e1f3541eef486881a3b60ed5be4965"
     }
    },
    "0f417848bd944590901805301d49054a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15a2b12901c246d882fd798609fceb69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d234f78da5f456e8fe4d85a8e129f9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25b50cb90d7e463ba8736ecfcf39626d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a982474fc59439c813fcb916599126f",
      "value": 7
     }
    },
    "2246a631405e4240a672208743eb1333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25b50cb90d7e463ba8736ecfcf39626d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fa5357ba5f847139d37efe457a93285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_346e498e9a474aceb4178797b0329929",
       "IPY_MODEL_1d234f78da5f456e8fe4d85a8e129f9d",
       "IPY_MODEL_b7e7ef816db44dafb4bf3bfeaa1a09c2"
      ],
      "layout": "IPY_MODEL_70eb92241b164feb83a373daa57118fe"
     }
    },
    "3276e963687c4e12b8c0f14a1535f014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2e304a7f220407bada34a20ac415424",
       "IPY_MODEL_f0a292d34c984da19bad182338fca1f9",
       "IPY_MODEL_4de16a026e9b46feb421bd269a681098"
      ],
      "layout": "IPY_MODEL_2246a631405e4240a672208743eb1333"
     }
    },
    "346e498e9a474aceb4178797b0329929": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9126d9ddf8524907b3825f6c348fb0d6",
      "placeholder": "​",
      "style": "IPY_MODEL_dd3cf04d4590446a9478427c4359eed5",
      "value": "Batches: 100%"
     }
    },
    "359a097446484f6b9f0a408b9a36e86d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49bc2f1c67cf419581d1082f34a02fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68a743a2ba724a38b9677dd467d1bf36",
      "placeholder": "​",
      "style": "IPY_MODEL_9c0fc847dda64fc1addeb687298a04f1",
      "value": " 7/7 [00:00&lt;00:00,  9.23it/s]"
     }
    },
    "4d004a9328a44af2afa5a1ea017ac5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de16a026e9b46feb421bd269a681098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68cabc74f8fc4dce9d274d0133676324",
      "placeholder": "​",
      "style": "IPY_MODEL_f53d67feef24424ea9eb56615ab58dcc",
      "value": " 299/299 [00:00&lt;00:00, 12975.92it/s]"
     }
    },
    "4e3dd8a6d46c4603b894e2c72aeefa18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e9b0386b84042649fafb3c1fc9eb2e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5786899f30e64e819074b00c3ee75103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa40d31a5c0d42ef93500c7134e9c4de",
       "IPY_MODEL_c3c244d98b084208ab61b181d18b7297",
       "IPY_MODEL_b28f8f1966014e8bb8969e542cfab395"
      ],
      "layout": "IPY_MODEL_359a097446484f6b9f0a408b9a36e86d"
     }
    },
    "58e4d3b1b6c14208b759f2c320a76f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a72a1b09ac14fd5bdcc97771494fdc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a982474fc59439c813fcb916599126f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "657e37ed3def4aef8b81544d797b5934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68a743a2ba724a38b9677dd467d1bf36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68cabc74f8fc4dce9d274d0133676324": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70eb92241b164feb83a373daa57118fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9126d9ddf8524907b3825f6c348fb0d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "974883c2255b4908a8b66646af6e2067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebf0f773fcbc4aa7ad597cdf8873a0fa",
      "placeholder": "​",
      "style": "IPY_MODEL_adf903e9a4d1435e8bf1c38cab061326",
      "value": "Batches: 100%"
     }
    },
    "9c0fc847dda64fc1addeb687298a04f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa40d31a5c0d42ef93500c7134e9c4de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f417848bd944590901805301d49054a",
      "placeholder": "​",
      "style": "IPY_MODEL_58e4d3b1b6c14208b759f2c320a76f39",
      "value": "Batches: 100%"
     }
    },
    "adf903e9a4d1435e8bf1c38cab061326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b28f8f1966014e8bb8969e542cfab395": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15a2b12901c246d882fd798609fceb69",
      "placeholder": "​",
      "style": "IPY_MODEL_5a72a1b09ac14fd5bdcc97771494fdc0",
      "value": " 10/10 [00:01&lt;00:00, 10.63it/s]"
     }
    },
    "b7e7ef816db44dafb4bf3bfeaa1a09c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d701cb9f7e3b414ba22c4960193018af",
      "placeholder": "​",
      "style": "IPY_MODEL_deba6a162cb645f497f38392f032f03a",
      "value": " 7/7 [00:00&lt;00:00,  9.32it/s]"
     }
    },
    "bc7b487ea278491ca16a1032d9ba5863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bcef0fa46e044c27ab0826adf73a60fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3c244d98b084208ab61b181d18b7297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffc7b241a64d4cec96db26f507912b72",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e3dd8a6d46c4603b894e2c72aeefa18",
      "value": 10
     }
    },
    "c3e1f3541eef486881a3b60ed5be4965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d701cb9f7e3b414ba22c4960193018af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3cf04d4590446a9478427c4359eed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "deba6a162cb645f497f38392f032f03a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8337d6b59004e0eb4f12f26ab28b546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf0f773fcbc4aa7ad597cdf8873a0fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0a292d34c984da19bad182338fca1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_657e37ed3def4aef8b81544d797b5934",
      "max": 299,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc7b487ea278491ca16a1032d9ba5863",
      "value": 299
     }
    },
    "f2e304a7f220407bada34a20ac415424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcef0fa46e044c27ab0826adf73a60fc",
      "placeholder": "​",
      "style": "IPY_MODEL_e8337d6b59004e0eb4f12f26ab28b546",
      "value": "Filtering: 100%"
     }
    },
    "f53d67feef24424ea9eb56615ab58dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc9d9a5ba2844a2e9d9040afc092f598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d004a9328a44af2afa5a1ea017ac5cf",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e9b0386b84042649fafb3c1fc9eb2e2",
      "value": 7
     }
    },
    "ffc7b241a64d4cec96db26f507912b72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
