{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae429fd-7c12-4113-9c12-208ef1c2fe09",
   "metadata": {},
   "source": [
    "# Lab for different llama models and inference with dataset to create summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "q5sjerXIZAr5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3290,
     "status": "ok",
     "timestamp": 1701171878050,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "q5sjerXIZAr5",
    "outputId": "cf127ac1-3a8b-4da8-a6ff-81e312e3fac2"
   },
   "outputs": [],
   "source": [
    "# # if run in colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# cd drive/MyDrive/PubPulse\n",
    "# pwd\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/PubPulse/lib')\n",
    "# sys.path.append('/content/drive/MyDrive/PubPulse/data')\n",
    "# sys.path.append('/content/drive/MyDrive/PubPulse/services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d467414",
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1701171878733,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "9d467414"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../lib\")\n",
    "sys.path.append(\"../data\")\n",
    "sys.path.append(\"../services\")\n",
    "sys.path.append(\"../models\")\n",
    "import importlib\n",
    "import time\n",
    "import preproc as pre\n",
    "importlib.reload(pre)\n",
    "import search_service as search\n",
    "importlib.reload(search)\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be4c22-4919-49e9-9629-128e7a59eac9",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d09951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre.make_dataset_from_txt('../data/extracted-text/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a295416",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1701171878736,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "0a295416",
    "outputId": "ae602b2a-d68f-436f-b7c0-405f9502bd6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fI-fcT-gac4H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1701171878738,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "fI-fcT-gac4H",
    "outputId": "2fd29550-f1a6-4ad8-ffd9-ac4396816c0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>input</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR CONTROLLED</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>SYSTEMIC RNA DELIVERY</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                        title  \\\n",
       "0         ABSTRACT                                     ABSTRACT   \n",
       "1  TITLE_PARAGRAPH                                 INTRODUCTION   \n",
       "2  TITLE_PARAGRAPH         DRUG DELIVERY SYSTEMS FOR CONTROLLED   \n",
       "3  TITLE_PARAGRAPH                        SYSTEMIC RNA DELIVERY   \n",
       "4  TITLE_PARAGRAPH  DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY   \n",
       "\n",
       "                                               input  \\\n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...   \n",
       "1  Medicine relies on the use of pharmacologicall...   \n",
       "2  RELEASE One important class of DDS is controll...   \n",
       "3  RNAs can manipulate gene expression through se...   \n",
       "4  One potential limitation to systemic administr...   \n",
       "\n",
       "                               data_source  \n",
       "0  Emerging Frontiers in Drug Delivery.txt  \n",
       "1  Emerging Frontiers in Drug Delivery.txt  \n",
       "2  Emerging Frontiers in Drug Delivery.txt  \n",
       "3  Emerging Frontiers in Drug Delivery.txt  \n",
       "4  Emerging Frontiers in Drug Delivery.txt  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c74e99c2-bd49-4ef2-9672-2f74330f0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['input_redact'] = dataset['input'].apply(pre.redact_specified_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d8a15d7-fa5b-4169-9215-d7478537433f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1701171878738,
     "user": {
      "displayName": "Anna Androvitsanea",
      "userId": "15906756172383653615"
     },
     "user_tz": -60
    },
    "id": "fI-fcT-gac4H",
    "outputId": "2fd29550-f1a6-4ad8-ffd9-ac4396816c0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>input</th>\n",
       "      <th>data_source</th>\n",
       "      <th>input_redact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR CONTROLLED</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>SYSTEMIC RNA DELIVERY</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                        title  \\\n",
       "0         ABSTRACT                                     ABSTRACT   \n",
       "1  TITLE_PARAGRAPH                                 INTRODUCTION   \n",
       "2  TITLE_PARAGRAPH         DRUG DELIVERY SYSTEMS FOR CONTROLLED   \n",
       "3  TITLE_PARAGRAPH                        SYSTEMIC RNA DELIVERY   \n",
       "4  TITLE_PARAGRAPH  DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY   \n",
       "\n",
       "                                               input  \\\n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...   \n",
       "1  Medicine relies on the use of pharmacologicall...   \n",
       "2  RELEASE One important class of DDS is controll...   \n",
       "3  RNAs can manipulate gene expression through se...   \n",
       "4  One potential limitation to systemic administr...   \n",
       "\n",
       "                               data_source  \\\n",
       "0  Emerging Frontiers in Drug Delivery.txt   \n",
       "1  Emerging Frontiers in Drug Delivery.txt   \n",
       "2  Emerging Frontiers in Drug Delivery.txt   \n",
       "3  Emerging Frontiers in Drug Delivery.txt   \n",
       "4  Emerging Frontiers in Drug Delivery.txt   \n",
       "\n",
       "                                        input_redact  \n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...  \n",
       "1  Medicine relies on the use of pharmacologicall...  \n",
       "2  RELEASE One important class of DDS is controll...  \n",
       "3  RNAs can manipulate gene expression through se...  \n",
       "4  One potential limitation to systemic administr...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f909b95f-5b9f-4c98-bfad-59bc8b5979be",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = ' '.join(dataset[(dataset.data_source == 'Emerging Frontiers in Drug Delivery.txt') &\\\n",
    "                                  (dataset.type != 'ABSTRACT')].input_redact.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f39f783-18cd-4909-8a2f-4b81ffb76458",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_pdf = pre.convert_pdf_to_txt('../data/raw-pdf/PMC8198544.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfe88181-e13c-4267-a670-363e85334a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = dataset[(dataset.data_source == 'Emerging Frontiers in Drug Delivery.txt') &\\\n",
    "                                  (dataset.type != 'ABSTRACT')].input_redact.tolist()\n",
    "paragraphs = [paragraph for paragraph in paragraphs if len(paragraph) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0645b431-0a19-412e-8abf-6ea13bcfe406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169\n",
      "962\n",
      "689\n",
      "2776\n",
      "691\n",
      "718\n",
      "2648\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "for paragraph in paragraphs:\n",
    "    print(len(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6fb4f5c-6acb-4c5d-a03d-3cc30938e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 1. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b970a42d-4037-4fe5-a446-f16b5f64cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_token_counts = [len(tokenizer.tokenize(text)) for text in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75236b8a-f7fe-4250-af62-c38b0d398336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre.plot_token_distribution(paragraphs_token_counts,\n",
    "                            \"Distribution of token per paragraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded816e-f130-4a87-b320-12f4fa9258b6",
   "metadata": {},
   "source": [
    "We must deal with the paragraphs that exceed the token limit of **512** the model `llama-2-7b.Q2_K.gguf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "537d0061-e6ac-4acd-a331-2a9eeb3a3781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting and filtering the paragraphs\n",
    "processed_paragraphs = pre.split_and_filter_paragraphs(paragraphs)\n",
    "\n",
    "processed_paragraphs_lengths = [len(p) for p in processed_paragraphs]\n",
    "pre.plot_token_distribution(processed_paragraphs_lengths,\n",
    "                            \"Distribution of token per processed paragraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f73e-4b3d-447c-9fe3-4c9ef16f4dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Llama-2-7b.Q2_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edf5770d-218a-474b-bc26-f4fda0e0db52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 73.56 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a34f0c31-1c68-4692-a5c1-1b388dbf3ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                  | 0/24 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      33.33 ms /    38 runs   (    0.88 ms per token,  1139.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73575.73 ms /   132 tokens (  557.39 ms per token,     1.79 tokens per second)\n",
      "llama_print_timings:        eval time =   27912.02 ms /    37 runs   (  754.38 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time =  101756.22 ms\n",
      "Summarizing paragraphs:   4%|                                       | 1/24 [01:41<39:00, 101.76s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tion. This summary of a comprehensive review article examines the current knowledge of the pharmacokinetic basis for the use of medication in children with cancer.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      71.23 ms /    74 runs   (    0.96 ms per token,  1038.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62871.51 ms /   114 tokens (  551.50 ms per token,     1.81 tokens per second)\n",
      "llama_print_timings:        eval time =   54925.22 ms /    73 runs   (  752.40 ms per token,     1.33 tokens per second)\n",
      "llama_print_timings:       total time =  118301.75 ms\n",
      "Summarizing paragraphs:   8%|                                     | 2/24 [03:40<40:52, 111.49s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ule filling is one of the most important unit operations performed at pharmaceutical plants where bulk powder substances are converted into dosage forms such as tablets, capsules, sachets etc. 2. Filling equipment usually consists of a hopper, filler head and trays for each product to be filled\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing paragraphs:  12%|                                   | 3/24 [05:49<41:53, 119.71s/it]llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =     144.47 ms /   150 runs   (    0.96 ms per token,  1038.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27933.99 ms /    41 tokens (  681.32 ms per token,     1.47 tokens per second)\n",
      "llama_print_timings:        eval time =  100458.57 ms /   149 runs   (  674.22 ms per token,     1.48 tokens per second)\n",
      "llama_print_timings:       total time =  129479.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2].  The primary purpose for using DDS is to prolong the residence time of an agent in its site of action by limiting its elimination from that site or by increasing its uptake by facilitating diffusion through tissue barriers (e.g., skin).  4.1.4 Intradermal Immunization Intradermal immunization is defined as the delivery of antigen to the skin. It is the preferred method for administration of vaccines in humans and has been used clinically since 1930, when it was first introduced for human poliomyelitis (type I) [1]. The current standardized device for intrad\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =       0.68 ms /     1 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64180.46 ms /   126 tokens (  509.37 ms per token,     1.96 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   64184.59 ms\n",
      "Summarizing paragraphs:  17%|                                   | 4/24 [06:53<32:35, 97.79s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      54.45 ms /    58 runs   (    0.94 ms per token,  1065.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62081.88 ms /   103 tokens (  602.74 ms per token,     1.66 tokens per second)\n",
      "llama_print_timings:        eval time =   33866.69 ms /    57 runs   (  594.15 ms per token,     1.68 tokens per second)\n",
      "llama_print_timings:       total time =   96306.13 ms\n",
      "Summarizing paragraphs:  21%|                                 | 5/24 [08:30<30:47, 97.26s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fundamental concepts behind DDS are inextricably linked to their chemical structure. 2. The physical shape and size of these materials can also affect them dramatically. 3. Targeting the body's immune system is an essential component of these materials.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /     1 runs   (    0.96 ms per token,  1040.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70845.35 ms /   144 tokens (  491.98 ms per token,     2.03 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   70852.15 ms\n",
      "Summarizing paragraphs:  25%|                               | 6/24 [09:40<26:29, 88.28s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "Summarizing paragraphs:  29%|                             | 7/24 [10:08<19:26, 68.60s/it]llama_print_timings:      sample time =       0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28082.86 ms /    56 tokens (  501.48 ms per token,     1.99 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   28086.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing paragraphs:  33%|                            | 8/24 [11:55<21:32, 80.80s/it]llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      99.44 ms /   150 runs   (    0.66 ms per token,  1508.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57969.39 ms /   119 tokens (  487.14 ms per token,     2.05 tokens per second)\n",
      "llama_print_timings:        eval time =   48311.84 ms /   149 runs   (  324.24 ms per token,     3.08 tokens per second)\n",
      "llama_print_timings:       total time =  106925.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polymer used in the study is composed of a 45:25:30 mole ratio of poly(D-Lactide), poly(-caprolactone) and poly(glycolic acid) (PLA, PCL, PGA) respectively. 2. The depot was created using a double injection method with a 10 mg BCNU dose per shot, into the right side of the abdominal cavity wall. The depot was placed on top of the lateral rectus muscle. 3. After three weeks there were no signs of inflammation around the implant site and the histology confirmed the lack of inflammatory reaction\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing paragraphs:  38%|                          | 9/24 [12:34<16:51, 67.45s/it]llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /     1 runs   (    0.67 ms per token,  1492.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38067.72 ms /   136 tokens (  279.91 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   38075.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      49.13 ms /    71 runs   (    0.69 ms per token,  1445.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48299.77 ms /   124 tokens (  389.51 ms per token,     2.57 tokens per second)\n",
      "llama_print_timings:        eval time =   25011.83 ms /    70 runs   (  357.31 ms per token,     2.80 tokens per second)\n",
      "llama_print_timings:       total time =   73633.39 ms\n",
      "Summarizing paragraphs:  42%|                        | 10/24 [13:47<16:11, 69.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP-10 is an excellent candidate for the delivery of targeted radionuclide therapy for the treatment of neuroendocrine tumors, both because its high affinity to somatostatin receptors and because it can be administered safely via a simple i. v. injection.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  46%|                      | 11/24 [14:45<14:17, 65.99s/it]llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      42.36 ms /    55 runs   (    0.77 ms per token,  1298.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31385.71 ms /   115 tokens (  272.92 ms per token,     3.66 tokens per second)\n",
      "llama_print_timings:        eval time =   26641.31 ms /    54 runs   (  493.36 ms per token,     2.03 tokens per second)\n",
      "llama_print_timings:       total time =   58347.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ease the temperature of a material by more than Tg and it becomes unstable. 2) The temperature will decrease by more than Tg (decrease in Tg) and the material is stable until annealed to room temperature.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "Summarizing paragraphs:  50%|                    | 12/24 [15:26<11:38, 58.23s/it]llama_print_timings:      sample time =       0.64 ms /     1 runs   (    0.64 ms per token,  1567.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40472.10 ms /   129 tokens (  313.74 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   40476.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  54%|                  | 13/24 [15:54<09:01, 49.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechanical properties of these hydrogels are designed to be more flexible than native tissue while retaining sufficient rigidity.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      20.60 ms /    29 runs   (    0.71 ms per token,  1407.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18756.50 ms /    68 tokens (  275.83 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9596.33 ms /    28 runs   (  342.73 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:       total time =   28488.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "Summarizing paragraphs:  58%|                 | 14/24 [16:27<07:21, 44.15s/it]llama_print_timings:      sample time =       0.68 ms /     1 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32429.71 ms /   124 tokens (  261.53 ms per token,     3.82 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   32433.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      34.09 ms /    51 runs   (    0.67 ms per token,  1495.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14502.56 ms /    56 tokens (  258.97 ms per token,     3.86 tokens per second)\n",
      "llama_print_timings:        eval time =   15815.81 ms /    50 runs   (  316.32 ms per token,     3.16 tokens per second)\n",
      "llama_print_timings:       total time =   30518.16 ms\n",
      "Summarizing paragraphs:  62%|               | 15/24 [16:57<06:00, 40.04s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utide, a glucagon-like peptide 1 receptor agonist with a long half-life in humans, could be an effective and safe alternative to current therapies for obesity treatment.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  67%|             | 16/24 [18:06<06:27, 48.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ological drug delivery system (DDS) has been used by scientists to treat disease for more than a century. However, the early years of this technology were marked by a high rate of failure and a low level of success. One reason is the lack of an effective delivery vehicle for many therapeutics. In addition, the development of the pharmaceutical industry in 1900\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      60.28 ms /    86 runs   (    0.70 ms per token,  1426.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35366.47 ms /   136 tokens (  260.05 ms per token,     3.85 tokens per second)\n",
      "llama_print_timings:        eval time =   32341.78 ms /    85 runs   (  380.49 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:       total time =   68100.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =       0.98 ms /     1 runs   (    0.98 ms per token,  1020.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17018.49 ms /    62 tokens (  274.49 ms per token,     3.64 tokens per second)\n",
      "Summarizing paragraphs:  71%|            | 17/24 [18:23<04:33, 39.03s/it]llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17025.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  75%|          | 18/24 [20:09<05:56, 59.37s/it]\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =     113.28 ms /   150 runs   (    0.76 ms per token,  1324.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38960.20 ms /   113 tokens (  344.78 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:        eval time =   66910.58 ms /   149 runs   (  449.06 ms per token,     2.23 tokens per second)\n",
      "llama_print_timings:       total time =  106707.29 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of the research article should not exceed more than two pages in length, excluding references, tables and figures. 2. Include a brief abstract (summary) of your study on page one; include the key elements (objective, design, setting, subjects, materials & instruments, procedures/interventions, outcomes). The summary should be written with complete sentences, paragraphs, and subheadings. 3. You must identify the research objective(s), rationale and clinical significance for your research project in the summary of the article. This will assist the examiner to evaluate your research proposal from an academic perspective. 4. The summary is not a copy-and-paste exercise: it needs\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      45.05 ms /    60 runs   (    0.75 ms per token,  1331.97 tokens per second)\n",
      "Summarizing paragraphs:  79%|        | 19/24 [21:15<05:06, 61.32s/it]llama_print_timings: prompt eval time =   37259.53 ms /   108 tokens (  345.00 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:        eval time =   28315.86 ms /    59 runs   (  479.93 ms per token,     2.08 tokens per second)\n",
      "llama_print_timings:       total time =   65875.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rked a year of firsts for molecular medicine. Several important milestones were realized, including the first FDA approval for a DNA-based therapeutic and the first intravesical chemotherapy treatment using biodegradable polymers.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =     132.61 ms /   119 runs   (    1.11 ms per token,   897.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62290.51 ms /   120 tokens (  519.09 ms per token,     1.93 tokens per second)\n",
      "llama_print_timings:        eval time =   99297.52 ms /   118 runs   (  841.50 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time =  162484.44 ms\n",
      "Summarizing paragraphs:  83%|      | 20/24 [23:58<06:06, 91.70s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential for tremendous clinical impact in the coming decades. Systemic delivery of RNAs can treat disease at the genetic level, seeking out aberrant regions of the body and repairing their function at the most basic level. Injectable materials can localize therapeutics to the site of action in order to mitigate off-target toxicity and increase clinical effect. Oral delivery of biologics can increase the indication and impact of this growing field of therapeutics. Extended release devices de.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      23.68 ms /    25 runs   (    0.95 ms per token,  1055.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65500.19 ms /   126 tokens (  519.84 ms per token,     1.92 tokens per second)\n",
      "llama_print_timings:        eval time =   16169.24 ms /    24 runs   (  673.72 ms per token,     1.48 tokens per second)\n",
      "llama_print_timings:       total time =   81836.82 ms\n",
      "Summarizing paragraphs:  88%|     | 21/24 [25:19<04:26, 88.74s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddition to the potential benefit to patients, this research may lead to novel therapeutic approaches.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =     159.11 ms /   150 runs   (    1.06 ms per token,   942.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   71612.11 ms /   118 tokens (  606.88 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:        eval time =  113539.38 ms /   149 runs   (  762.01 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time =  186299.13 ms\n",
      "Summarizing paragraphs:  92%|   | 22/24 [28:26<03:56, 118.02s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-Cas9 is a simple genome editing technology that uses small RNAs as homing endonucleases to target DNA at specific genomic loci and generate double-strand breaks (DSB). 2) Zinc finger nucleases (ZFNs) are artificially generated protein/DNA complexes, which can be programmed to recognize a specific genome sequence and mediate DSB. 3) Transcription activator like effector nuclease (TALEN) technology employs the same principle as ZFN but uses different binding domains in order to generate an entirely new class of genome editing tools. 4) Most approaches require the delivery of two components\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =      47.34 ms /    43 runs   (    1.10 ms per token,   908.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20813.41 ms /    39 tokens (  533.68 ms per token,     1.87 tokens per second)\n",
      "llama_print_timings:        eval time =   35233.59 ms /    42 runs   (  838.90 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time =   56403.08 ms\n",
      "Summarizing paragraphs:  96%| | 23/24 [29:22<01:39, 99.53s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 - Cochrane Library (Stable URL) 1508201900 - PubMed Central (PMC, ftp)\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs: 100%|| 24/24 [31:33<00:00, 108.94s/it]\n",
      "llama_print_timings:        load time =   73576.02 ms\n",
      "llama_print_timings:      sample time =     127.58 ms /   150 runs   (    0.85 ms per token,  1175.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46051.98 ms /    69 tokens (  667.42 ms per token,     1.50 tokens per second)\n",
      "llama_print_timings:        eval time =   83958.41 ms /   149 runs   (  563.48 ms per token,     1.77 tokens per second)\n",
      "llama_print_timings:       total time =  130886.71 ms\n",
      "Summarizing paragraphs: 100%|| 24/24 [31:33<00:00, 78.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plans are a popular retirement savings vehicle in the United States. In contrast to defined benefit pension plans, individuals save for their own retirement through such vehicles as individual retirement accounts or 401(k)-type plans with employer contributions and tax relief on investment earnings. Our study is the first to examine the long-term impact of employers' offering a 401(k) plan as an employee benefit in the United States. We find that adoption of 401(k) pension plans by firms was associated with a reduction in mortality risk for workers employed at establishments where 401(k)-type plans were adopted relative to\n",
      "-----------\n",
      "tion. This summary of a comprehensive review article examines the current knowledge of the pharmacokinetic basis for the use of medication in children with cancer. ule filling is one of the most important unit operations performed at pharmaceutical plants where bulk powder substances are converted into dosage forms such as tablets, capsules, sachets etc. 2. Filling equipment usually consists of a hopper, filler head and trays for each product to be filled 1,2].  The primary purpose for using DDS is to prolong the residence time of an agent in its site of action by limiting its elimination from that site or by increasing its uptake by facilitating diffusion through tissue barriers (e.g., skin).  4.1.4 Intradermal Immunization Intradermal immunization is defined as the delivery of antigen to the skin. It is the preferred method for administration of vaccines in humans and has been used clinically since 1930, when it was first introduced for human poliomyelitis (type I) [1]. The current standardized device for intrad  fundamental concepts behind DDS are inextricably linked to their chemical structure. 2. The physical shape and size of these materials can also affect them dramatically. 3. Targeting the body's immune system is an essential component of these materials.   polymer used in the study is composed of a 45:25:30 mole ratio of poly(D-Lactide), poly(-caprolactone) and poly(glycolic acid) (PLA, PCL, PGA) respectively. 2. The depot was created using a double injection method with a 10 mg BCNU dose per shot, into the right side of the abdominal cavity wall. The depot was placed on top of the lateral rectus muscle. 3. After three weeks there were no signs of inflammation around the implant site and the histology confirmed the lack of inflammatory reaction  IP-10 is an excellent candidate for the delivery of targeted radionuclide therapy for the treatment of neuroendocrine tumors, both because its high affinity to somatostatin receptors and because it can be administered safely via a simple i. v. injection. ease the temperature of a material by more than Tg and it becomes unstable. 2) The temperature will decrease by more than Tg (decrease in Tg) and the material is stable until annealed to room temperature.  mechanical properties of these hydrogels are designed to be more flexible than native tissue while retaining sufficient rigidity.  utide, a glucagon-like peptide 1 receptor agonist with a long half-life in humans, could be an effective and safe alternative to current therapies for obesity treatment. ological drug delivery system (DDS) has been used by scientists to treat disease for more than a century. However, the early years of this technology were marked by a high rate of failure and a low level of success. One reason is the lack of an effective delivery vehicle for many therapeutics. In addition, the development of the pharmaceutical industry in 1900  summary of the research article should not exceed more than two pages in length, excluding references, tables and figures. 2. Include a brief abstract (summary) of your study on page one; include the key elements (objective, design, setting, subjects, materials & instruments, procedures/interventions, outcomes). The summary should be written with complete sentences, paragraphs, and subheadings. 3. You must identify the research objective(s), rationale and clinical significance for your research project in the summary of the article. This will assist the examiner to evaluate your research proposal from an academic perspective. 4. The summary is not a copy-and-paste exercise: it needs rked a year of firsts for molecular medicine. Several important milestones were realized, including the first FDA approval for a DNA-based therapeutic and the first intravesical chemotherapy treatment using biodegradable polymers. potential for tremendous clinical impact in the coming decades. Systemic delivery of RNAs can treat disease at the genetic level, seeking out aberrant regions of the body and repairing their function at the most basic level. Injectable materials can localize therapeutics to the site of action in order to mitigate off-target toxicity and increase clinical effect. Oral delivery of biologics can increase the indication and impact of this growing field of therapeutics. Extended release devices de. ddition to the potential benefit to patients, this research may lead to novel therapeutic approaches. PR-Cas9 is a simple genome editing technology that uses small RNAs as homing endonucleases to target DNA at specific genomic loci and generate double-strand breaks (DSB). 2) Zinc finger nucleases (ZFNs) are artificially generated protein/DNA complexes, which can be programmed to recognize a specific genome sequence and mediate DSB. 3) Transcription activator like effector nuclease (TALEN) technology employs the same principle as ZFN but uses different binding domains in order to generate an entirely new class of genome editing tools. 4) Most approaches require the delivery of two components 900 - Cochrane Library (Stable URL) 1508201900 - PubMed Central (PMC, ftp) plans are a popular retirement savings vehicle in the United States. In contrast to defined benefit pension plans, individuals save for their own retirement through such vehicles as individual retirement accounts or 401(k)-type plans with employer contributions and tax relief on investment earnings. Our study is the first to examine the long-term impact of employers' offering a 401(k) plan as an employee benefit in the United States. We find that adoption of 401(k) pension plans by firms was associated with a reduction in mortality risk for workers employed at establishments where 401(k)-type plans were adopted relative to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Correctly format the prompt with the current paragraph\n",
    "    formatted_prompt = \"Q: Create a summary of this {}. Summary: \".format(paragraph)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,\n",
    "        stop=[\"Q:\", \"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in minutes\n",
    "elapsed_time_minutes = (end_time - start_time) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77a70b9a-9d83-48f3-a889-d39f1b90f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  11%|                                     | 1/9 [01:42<13:41, 102.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure).\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      73.10 ms /   117 runs   (    0.62 ms per token,  1600.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64469.50 ms /   283 tokens (  227.81 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =   37758.36 ms /   116 runs   (  325.50 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =  102661.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  22%|                                 | 2/9 [02:37<08:41, 74.55s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.66 ms /     1 runs   (    0.66 ms per token,  1508.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54860.92 ms /   224 tokens (  244.91 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   54866.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  33%|                            | 3/9 [03:24<06:11, 61.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /     1 runs   (    0.67 ms per token,  1494.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46869.93 ms /   193 tokens (  242.85 ms per token,     4.12 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   46873.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  44%|                        | 4/9 [04:46<05:49, 69.94s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.66 ms /     1 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82226.65 ms /   342 tokens (  240.43 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   82230.01 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  56%|                   | 5/9 [06:06<04:54, 73.52s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79876.43 ms /   332 tokens (  240.59 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   79880.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:  67%|              | 6/9 [06:52<03:12, 64.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     1 runs   (    0.91 ms per token,  1101.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45816.23 ms /   185 tokens (  247.66 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   45821.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  78%|         | 7/9 [08:00<02:11, 65.53s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      28.55 ms /    50 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52303.81 ms /   198 tokens (  264.16 ms per token,     3.79 tokens per second)\n",
      "llama_print_timings:        eval time =   15950.22 ms /    49 runs   (  325.51 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =   68463.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the adult population has prediabetic hyperglycemia (PG); most PG do not progress to diabetes, which is associated with significant morbidity and premature mortality. 1\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      60.25 ms /    86 runs   (    0.70 ms per token,  1427.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   86531.95 ms /   322 tokens (  268.73 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =   32954.18 ms /    85 runs   (  387.70 ms per token,     2.58 tokens per second)\n",
      "llama_print_timings:       total time =  119877.82 ms\n",
      "Summarizing paragraphs:  89%|    | 8/9 [10:00<01:22, 82.84s/it]Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future of medicine will increasingly rely on drugs that target underlying genetic pathways. 2. Emerging platforms for RNA-based therapy have the potential to treat disease at a genetic level. 3. Oral administration of biologic medicines requires advanced drug delivery systems in order to deliver therapeutics to their site of action while minimizing off-target effects.\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =     112.55 ms /   150 runs   (    0.75 ms per token,  1332.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80824.88 ms /   274 tokens (  294.98 ms per token,     3.39 tokens per second)\n",
      "llama_print_timings:        eval time =   66824.69 ms /   149 runs   (  448.49 ms per token,     2.23 tokens per second)\n",
      "llama_print_timings:       total time =  148429.00 ms\n",
      "Summarizing paragraphs: 100%|| 9/9 [12:29<00:00, 83.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharmacological and genetic approaches to treating chronic disease, including the development of gene therapy medications, require an improved understanding of the interaction between drug delivery vehicles and molecular targets in the body; 2) living systems can be re-engineered to work with the body, and not against, to treat disease using the outstanding delivery mechanisms of microvesicles, pathogens, and cells (e.g., selective targeting, prolonged circulation, and immune tolerance); 3) these delivery mechanisms require the development of improved materials for the safe delivery of gene editing technologies; and 4) advanced delivery methods will likely improve as we understand how biological path\n",
      "-----------\n",
      "elivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure).      of the adult population has prediabetic hyperglycemia (PG); most PG do not progress to diabetes, which is associated with significant morbidity and premature mortality. 1 future of medicine will increasingly rely on drugs that target underlying genetic pathways. 2. Emerging platforms for RNA-based therapy have the potential to treat disease at a genetic level. 3. Oral administration of biologic medicines requires advanced drug delivery systems in order to deliver therapeutics to their site of action while minimizing off-target effects. pharmacological and genetic approaches to treating chronic disease, including the development of gene therapy medications, require an improved understanding of the interaction between drug delivery vehicles and molecular targets in the body; 2) living systems can be re-engineered to work with the body, and not against, to treat disease using the outstanding delivery mechanisms of microvesicles, pathogens, and cells (e.g., selective targeting, prolonged circulation, and immune tolerance); 3) these delivery mechanisms require the development of improved materials for the safe delivery of gene editing technologies; and 4) advanced delivery methods will likely improve as we understand how biological path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(filtered_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Correctly format the prompt with the current paragraph\n",
    "    formatted_prompt = \"Q: Create a summary of this {}. Summary: \".format(paragraph)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,\n",
    "        stop=[\"Q:\", \"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1da7ac8-2615-4a1f-9bf3-e8f869540efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                   | 0/9 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  11%|                                      | 1/9 [01:15<10:06, 75.80s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /     4 runs   (    0.73 ms per token,  1368.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74671.63 ms /   276 tokens (  270.55 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1104.65 ms /     3 runs   (  368.22 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:       total time =   75796.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  22%|                                 | 2/9 [02:30<08:46, 75.28s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      15.94 ms /    24 runs   (    0.66 ms per token,  1506.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65224.73 ms /   220 tokens (  296.48 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:        eval time =    9572.27 ms /    23 runs   (  416.19 ms per token,     2.40 tokens per second)\n",
      "llama_print_timings:       total time =   74907.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  33%|                            | 3/9 [03:36<07:06, 71.04s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      21.12 ms /    32 runs   (    0.66 ms per token,  1515.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54610.65 ms /   189 tokens (  288.95 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:        eval time =   11238.20 ms /    31 runs   (  362.52 ms per token,     2.76 tokens per second)\n",
      "llama_print_timings:       total time =   65986.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  44%|                        | 4/9 [05:43<07:45, 93.19s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      48.77 ms /    76 runs   (    0.64 ms per token,  1558.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   99298.29 ms /   338 tokens (  293.78 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:        eval time =   27508.22 ms /    75 runs   (  366.78 ms per token,     2.73 tokens per second)\n",
      "llama_print_timings:       total time =  127140.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  56%|                   | 5/9 [07:18<06:14, 93.62s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1519.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   94007.37 ms /   328 tokens (  286.61 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:        eval time =     378.44 ms /     1 runs   (  378.44 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =   94394.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      37.44 ms /    52 runs   (    0.72 ms per token,  1388.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51814.44 ms /   181 tokens (  286.27 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:        eval time =   19349.07 ms /    51 runs   (  379.39 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =   71401.06 ms\n",
      "Summarizing paragraphs:  67%|              | 6/9 [08:29<04:18, 86.07s/it]Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  78%|         | 7/9 [10:00<02:55, 87.67s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      54.30 ms /    68 runs   (    0.80 ms per token,  1252.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59581.95 ms /   194 tokens (  307.12 ms per token,     3.26 tokens per second)\n",
      "llama_print_timings:        eval time =   31006.99 ms /    67 runs   (  462.79 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time =   90968.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  89%|    | 8/9 [11:50<01:34, 94.68s/it]\n",
      "llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      17.36 ms /    25 runs   (    0.69 ms per token,  1440.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101044.90 ms /   318 tokens (  317.75 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:        eval time =    8513.52 ms /    24 runs   (  354.73 ms per token,     2.82 tokens per second)\n",
      "llama_print_timings:       total time =  109671.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs: 100%|| 9/9 [13:33<00:00, 97.43s/it]llama_print_timings:        load time =   64469.97 ms\n",
      "llama_print_timings:      sample time =      35.72 ms /    41 runs   (    0.87 ms per token,  1147.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82434.99 ms /   270 tokens (  305.31 ms per token,     3.28 tokens per second)\n",
      "llama_print_timings:        eval time =   20775.58 ms /    40 runs   (  519.39 ms per token,     1.93 tokens per second)\n",
      "llama_print_timings:       total time =  103473.97 ms\n",
      "Summarizing paragraphs: 100%|| 9/9 [13:33<00:00, 90.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1). These observations have major implications for the design and development of new drugs and the understanding of old ones. For example, a mutated form of the mRNA for beta-globin may be used to treat sickle-cell anemia. A potential strategy for improving these devices is to coat them with biodegradable DDS that control release of drugs for up to several months. This strategy has been used with some success in clinical trials and may offer significant advantages over stents alone, especially when the drug being released is a thrombolytic or antirestenosis agent. Thus, to prolong the residence time and enhance bioavailability of a drug, extended-release technology can be applied. This is an important strategy for biotherapeutics that are poorly absorbed or rapidly cleared from the bloodstream. 208 However, the harvesting, purification, and administration of these microvesicles are technically challenging for clinical applications. 209 In particular, controlling the size range and specificity of the microvesicle cargo is difficult to achieve in an efficient manner (Figure 15). The combination of advanced drug delivery technologies with molecular biology will be essential to treat disease at the root cause. In particular, polymers, aptamers, peptides, and nanofibers have all been shown to efficiently enter cells when combined with genetic editing reagents (97).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(filtered_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c6b6ed-ed69-4dc3-9a71-3b467e6244b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  750.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 123.75 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:   0%|                                                   | 0/8 [00:00<?, ?it/s]\n",
      "Summarizing paragraphs:  12%|                                    | 1/8 [02:00<14:02, 120.42s/it]llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =      30.66 ms /    44 runs   (    0.70 ms per token,  1435.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101522.86 ms /   277 tokens (  366.51 ms per token,     2.73 tokens per second)\n",
      "llama_print_timings:        eval time =   18678.72 ms /    43 runs   (  434.39 ms per token,     2.30 tokens per second)\n",
      "llama_print_timings:       total time =  120414.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  25%|                                | 2/8 [03:14<09:18, 93.13s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       7.94 ms /    12 runs   (    0.66 ms per token,  1512.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70068.22 ms /   220 tokens (  318.49 ms per token,     3.14 tokens per second)\n",
      "llama_print_timings:        eval time =    3893.37 ms /    11 runs   (  353.94 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   74020.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  38%|                          | 3/8 [04:15<06:31, 78.34s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1365.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60383.97 ms /   189 tokens (  319.49 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:        eval time =     354.95 ms /     1 runs   (  354.95 ms per token,     2.82 tokens per second)\n",
      "llama_print_timings:       total time =   60748.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  62%|                | 5/8 [05:05<02:24, 48.19s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1520.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49819.24 ms /   181 tokens (  275.24 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:        eval time =     319.29 ms /     1 runs   (  319.29 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:       total time =   50147.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  75%|          | 6/8 [06:14<01:48, 54.12s/it]\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =      34.54 ms /    51 runs   (    0.68 ms per token,  1476.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51667.20 ms /   194 tokens (  266.33 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:        eval time =   16911.76 ms /    50 runs   (  338.24 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =   68793.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs: 100%|| 8/8 [06:39<00:00, 49.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1). In other words, DDS are custom-built to ensure that each drug is delivered in a therapeutic effective dose at a targeted site where it will exert its effect. The next section provides an overview of these concepts. For example, the islet beta-cells in type I diabetics have been replaced with insulin-producing cells derived from stem cells through microvesicle isolation and purification for autologous therapy (Figure The remaining authors have declared no interests with commercial supporters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "llama_print_timings:        load time =  101523.53 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    14 runs   (    0.61 ms per token,  1649.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20975.83 ms /    78 tokens (  268.92 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4588.45 ms /    13 runs   (  352.96 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   25622.28 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_m = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "           n_ctx=1500, n_gpu_layers=-1)\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_m(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8783ffc9-c043-462c-b184-5833360c241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 600\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  300.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 73.56 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:  12%|                                     | 1/8 [01:27<10:15, 87.89s/it]\n",
      "llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =       2.57 ms /     4 runs   (    0.64 ms per token,  1558.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   86908.32 ms /   269 tokens (  323.08 ms per token,     3.10 tokens per second)\n",
      "llama_print_timings:        eval time =     960.12 ms /     3 runs   (  320.04 ms per token,     3.12 tokens per second)\n",
      "llama_print_timings:       total time =   87887.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  25%|                                | 2/8 [03:04<09:18, 93.12s/it]\n",
      "llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =      70.24 ms /   102 runs   (    0.69 ms per token,  1452.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59710.01 ms /   218 tokens (  273.90 ms per token,     3.65 tokens per second)\n",
      "llama_print_timings:        eval time =   36596.99 ms /   101 runs   (  362.35 ms per token,     2.76 tokens per second)\n",
      "llama_print_timings:       total time =   96780.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  38%|                          | 3/8 [04:34<07:39, 91.82s/it]llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =      38.27 ms /    39 runs   (    0.98 ms per token,  1019.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64828.73 ms /   189 tokens (  343.01 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:        eval time =   25171.42 ms /    38 runs   (  662.41 ms per token,     1.51 tokens per second)\n",
      "llama_print_timings:       total time =   90270.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  62%|                | 5/8 [06:09<03:19, 66.63s/it]llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =       1.85 ms /     2 runs   (    0.93 ms per token,  1078.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   93941.94 ms /   170 tokens (  552.60 ms per token,     1.81 tokens per second)\n",
      "llama_print_timings:        eval time =     670.36 ms /     1 runs   (  670.36 ms per token,     1.49 tokens per second)\n",
      "llama_print_timings:       total time =   94624.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =      73.47 ms /    71 runs   (    1.03 ms per token,   966.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83949.37 ms /   186 tokens (  451.34 ms per token,     2.22 tokens per second)\n",
      "llama_print_timings:        eval time =   61321.03 ms /    70 runs   (  876.01 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:       total time =  145840.50 ms\n",
      "Summarizing paragraphs:  75%|          | 6/8 [08:35<02:58, 89.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   86909.07 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     1 runs   (    0.91 ms per token,  1100.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41468.23 ms /    65 tokens (  637.97 ms per token,     1.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   41475.97 ms\n",
      "Summarizing paragraphs: 100%|| 8/8 [09:16<00:00, 69.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1). This is called active translocation or active uptake by receptors or enzymes that are usually found on the cell surface. Fourth, a multitude of drug transport pathways exist in cells and organs, which determine the fate of DDS after ingestion or injection. The concept of permeability is fundamental to drug delivery, where the uptake and release of macromolecules into or out of tissues are controlled by transporters on the cell surface. They can also be used for vaccines, which would enable the use of non-infectious viruses to carry out these functions, thereby avoiding concerns over contagion. The use of microvesicles for the delivery of therapeutics has been suggested as an approach that avoids several limitations associated with the more traditional approaches of gene therapy using viral vectors, plasmids and DNA-protein complexes (e.g., naked DNA or DNA in cationic liposomes).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm_m = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "           n_ctx=600, n_gpu_layers=-1)\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_m(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in minutes\n",
    "elapsed_time_minutes = (end_time - start_time) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232e878b-bf9c-4ef9-a819-2cb53edca5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  250.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 71.91 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_l = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "              n_ctx=500, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eabb1bab-fa93-4989-9ad3-91ea81a5abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 1. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "tokens_combined_text = tokenizer.tokenize(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11505b63-a391-479e-8f2a-c409ca29a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   4%|                                        | 1/24 [00:31<12:00, 31.33s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     1 runs   (    0.65 ms per token,  1529.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31312.92 ms /   134 tokens (  233.68 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   31318.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:   8%|                                      | 2/24 [01:23<15:55, 43.41s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      63.01 ms /    99 runs   (    0.64 ms per token,  1571.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23624.82 ms /   109 tokens (  216.74 ms per token,     4.61 tokens per second)\n",
      "llama_print_timings:        eval time =   27897.62 ms /    98 runs   (  284.67 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   51865.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  12%|                                    | 3/24 [01:49<12:30, 35.72s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      40.55 ms /    65 runs   (    0.62 ms per token,  1603.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8376.86 ms /    38 tokens (  220.44 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:        eval time =   17966.03 ms /    64 runs   (  280.72 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =   26567.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  17%|                                   | 4/24 [02:27<12:07, 36.39s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      22.95 ms /    37 runs   (    0.62 ms per token,  1612.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27033.97 ms /   123 tokens (  219.79 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:        eval time =   10242.12 ms /    36 runs   (  284.50 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   37402.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  21%|                                 | 5/24 [03:02<11:22, 35.91s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      28.28 ms /    46 runs   (    0.61 ms per token,  1626.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22069.78 ms /   100 tokens (  220.70 ms per token,     4.53 tokens per second)\n",
      "llama_print_timings:        eval time =   12843.51 ms /    45 runs   (  285.41 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =   35070.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  25%|                               | 6/24 [04:17<14:46, 49.27s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      90.45 ms /   150 runs   (    0.60 ms per token,  1658.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31742.35 ms /   139 tokens (  228.36 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:        eval time =   42915.27 ms /   149 runs   (  288.02 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   75206.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  29%|                             | 7/24 [04:54<12:48, 45.23s/it]llama_print_timings:      sample time =      57.47 ms /    88 runs   (    0.65 ms per token,  1531.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11775.20 ms /    52 tokens (  226.45 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =   24810.96 ms /    87 runs   (  285.18 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   36904.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  33%|                            | 8/24 [05:29<11:11, 41.98s/it]llama_print_timings:      sample time =      18.69 ms /    32 runs   (    0.58 ms per token,  1711.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25983.07 ms /   114 tokens (  227.92 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =    8930.64 ms /    31 runs   (  288.09 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   35024.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  38%|                          | 9/24 [05:59<09:33, 38.25s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.64 ms /     1 runs   (    0.64 ms per token,  1557.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30047.05 ms /   132 tokens (  227.63 ms per token,     4.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   30050.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  42%|                        | 10/24 [06:38<08:58, 38.43s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      24.99 ms /    38 runs   (    0.66 ms per token,  1520.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27846.07 ms /   123 tokens (  226.39 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =   10846.97 ms /    37 runs   (  293.16 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =   38831.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  46%|                      | 11/24 [07:14<08:09, 37.63s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      24.92 ms /    39 runs   (    0.64 ms per token,  1565.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24660.97 ms /   108 tokens (  228.34 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:        eval time =   10994.62 ms /    38 runs   (  289.33 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   35799.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  50%|                    | 12/24 [08:11<08:43, 43.60s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      67.32 ms /   100 runs   (    0.67 ms per token,  1485.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27901.11 ms /   122 tokens (  228.70 ms per token,     4.37 tokens per second)\n",
      "llama_print_timings:        eval time =   28981.87 ms /    99 runs   (  292.75 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =   57263.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  54%|                  | 13/24 [08:29<06:33, 35.76s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.62 ms /     1 runs   (    0.62 ms per token,  1602.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17693.15 ms /    76 tokens (  232.80 ms per token,     4.30 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17696.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  58%|                 | 14/24 [09:07<06:05, 36.58s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      26.61 ms /    40 runs   (    0.67 ms per token,  1503.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27059.91 ms /   117 tokens (  231.28 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:        eval time =   11262.42 ms /    39 runs   (  288.78 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   38468.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  62%|               | 15/24 [09:42<05:24, 36.00s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      44.17 ms /    68 runs   (    0.65 ms per token,  1539.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15166.97 ms /    65 tokens (  233.34 ms per token,     4.29 tokens per second)\n",
      "llama_print_timings:        eval time =   19256.82 ms /    67 runs   (  287.42 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:       total time =   34671.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  67%|             | 16/24 [10:39<05:39, 42.43s/it]llama_print_timings:      sample time =      56.43 ms /    87 runs   (    0.65 ms per token,  1541.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31470.75 ms /   135 tokens (  233.12 ms per token,     4.29 tokens per second)\n",
      "llama_print_timings:        eval time =   25538.44 ms /    86 runs   (  296.96 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:       total time =   57336.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  71%|            | 17/24 [11:37<05:30, 47.22s/it]llama_print_timings:      sample time =      99.48 ms /   150 runs   (    0.66 ms per token,  1507.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14146.97 ms /    60 tokens (  235.78 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =   43633.49 ms /   149 runs   (  292.84 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =   58352.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs:  75%|          | 18/24 [12:13<04:22, 43.68s/it]llama_print_timings:      sample time =      21.95 ms /    34 runs   (    0.65 ms per token,  1549.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25685.03 ms /   108 tokens (  237.82 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9630.66 ms /    33 runs   (  291.84 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =   35438.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  79%|        | 19/24 [12:58<03:40, 44.18s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      47.72 ms /    72 runs   (    0.66 ms per token,  1508.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24328.25 ms /   103 tokens (  236.20 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:        eval time =   20748.74 ms /    71 runs   (  292.24 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =   45341.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  83%|      | 20/24 [13:40<02:54, 43.52s/it]llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =      33.21 ms /    51 runs   (    0.65 ms per token,  1535.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27108.05 ms /   115 tokens (  235.72 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =   14693.68 ms /    50 runs   (  293.87 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time =   41997.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  88%|     | 21/24 [14:10<01:57, 39.31s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /     5 runs   (    0.65 ms per token,  1537.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28318.07 ms /   122 tokens (  232.12 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1154.87 ms /     4 runs   (  288.72 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   29491.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  92%|   | 22/24 [14:40<01:13, 36.75s/it]\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       9.32 ms /    15 runs   (    0.62 ms per token,  1609.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26627.08 ms /   113 tokens (  235.64 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =    4074.32 ms /    14 runs   (  291.02 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =   30755.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "llama_print_timings:      sample time =       0.59 ms /     1 runs   (    0.59 ms per token,  1700.68 tokens per second)\n",
      "Summarizing paragraphs:  96%| | 23/24 [14:49<00:28, 28.24s/it]llama_print_timings: prompt eval time =    8387.58 ms /    35 tokens (  239.65 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    8391.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31314.65 ms\n",
      "Summarizing paragraphs: 100%|| 24/24 [15:16<00:00, 28.05s/it]llama_print_timings:      sample time =      18.92 ms /    31 runs   (    0.61 ms per token,  1638.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18390.38 ms /    78 tokens (  235.77 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9102.08 ms /    30 runs   (  303.40 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:       total time =   27612.19 ms\n",
      "Summarizing paragraphs: 100%|| 24/24 [15:16<00:00, 38.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue-specific time. In addition, many approaches have been applied to limit the release of drugs to a particular environment or location. These strategies include the use of biodegradable polymers and liposomes that are able to degrade in situ at desired rates. The chemical strategy can also be used to control the rate of drug release in vivo by altering physicochemical properties, such as pH, osmolality, and viscosity. 2.  6.4). DDS can be classified by their release mechanism, which involves either passive diffusion into the bloodstream or active transport across a biological membrane, such as a cellular wall, using transporters that are specific to a particular molecule (13). ues to design controlled release systems. These cues include the shape of the drug, its physicochemical properties, and the release rate required by the targeted application. For example, oral drug delivery systems may interact with enzymes in the GI tract before reaching their target organ, which is important for understanding and predicting the pharmacokinetics of many drugs. untion, and infection [5254]. To target RNAs for degradation in this tissue, a library of siRNA was first designed around an Fc-fragment derived from the antibody Fab region (Fig. 3a) [55] that targets specific serum albumin isotypes overexpressed in liver cirrhosis and hepatocellular carcinoma . A liposomal delivery system was used to transfect the cells with siRNA targeted to these isotypes (Fig. 3b) [57]. When transfected into hepatocytes, the albumin siRNAs caused reduced production of sg Moreover, mRNA technology is a universal platform that allows for the expression of any target protein, even if its nucleotide sequence has not yet been fully characterized. In this chapter we describe the different mRNA technologies used to express human proteins in various biological settings. We discuss the challenges associated with the translation and delivery of these therapeutics and summarize the current state of the field. -d carbonate particles in rats with ovarian cancer has resulted in significantly higher therapeutic concentrations than those achieved systemically. ection, In Situ Forming (ISF) is an emerging approach that combines hydrogels and drug delivery systems to form drug-loaded depots within tissue. ps. As the temperature is decreased from above body temperature, thermally responsive hydrogels may rapidly undergo a transition from a fluid phase to a solid phase (Figure ability, we are designing a class of polymers that can be injected into any tissue of choice. These polymers will have high molecular weight (MW) to maintain rigidity and prevent precipitation during normal operation. The MW also determines the rate at which the polymer dissolves, which is controlled by the solubility parameter S(r), where r is the distance between two atoms in a chain of repeating monomers (Figure 1). administrable biologic (NDS-001) that is a monoclonal antibody directed against interferon- (IFN-). The pharmacokinetics of semaglutide are linear over a dose range of 1.5-15 mg once-weekly and 7.4-74 mg once-daily for 2 wks, with mean half life of approximately 60 min [9]. rosstalk among different cell types, to regulate signaling pathways, to participate in innate immunity, and to deliver cargo (e.g., proteins, lipids, nucleic acids) to various organs and tissues. 208210 These natural drug carriers are of interest as vehicles for delivery of therapeutics to specific locations within the body. Several different types of human tissue are used in such procedures: hematopoietic stem cells (HSC) for blood formation; mesenchymal stem cells (MSC) for bone, cartilage and marrow repair; and endothelial or other precursor cells to facilitate vascularization. In addition, patient-derived tumor cells can be used for genetic modification by gene transfer or transgenic construction in the context of gene therapy. The development of new methods and reagents has made this field more attractive than ever, but major technical issues remain to be addressed, including (1) the isolation of functional HSCs from bone marrow and other t ances in polymer chemistry, physical properties, drug formulation and processing methods, and delivery technologies to address the ever-increasing needs of patients. ended to receive little attention from scientists working in the field of pharmaceutical engineering and formulation science. We hope, by outlining what is possible today and highlighting promising techniques on the horizon, that you will begin to consider how you might contribute your skills, experience and expertise toward this growing field of drug delivery research. -risk development and allow for long-acting treatments with decreased side effects. These are just a few examples of how nanotechnology is revolutionizing medicine, and its impact on drug delivery will be felt across the globe. 2542 the nuclear envelope. The article is accompanied by a news release. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # # Skip paragraphs that are too long\n",
    "    # if len(tokens) > 512:\n",
    "    #     continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_l(\n",
    "        formatted_prompt,\n",
    "        max_tokens=150,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb5c38-4e85-4b1b-b0da-824be67bdb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 73.56 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:   0%|                                                   | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\")\n",
    "summary = []\n",
    "for paragraph in tqdm(paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Format the prompt with the individual paragraph\n",
    "    formatted_prompt = \"\"\"Summarize the following text, include only full sentences, \n",
    "                           don't include stuff like Table, Figure, \n",
    "                           create fluent text that can be merged with other snippets: {}\\nSummary:\"\"\".format(paragraph)\n",
    "\n",
    "    # formatted_prompt = \"\"\"Summarize the following text, include only full sentences, \n",
    "    #                        don't include stuff like Table, Figure, \n",
    "    #                        create fluent text that can be merged with other snippets: {}\\nSummary:\"\"\".format(paragraph)\n",
    "    \n",
    "    # Generate summary\n",
    "    output = llm(\n",
    "        formatted_prompt,\n",
    "        max_tokens=200,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    \n",
    "    generated_text = output['choices'][0]['text']\n",
    "    summary_index = generated_text.find(\"Summary:\")\n",
    "    summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "    \n",
    "    print(summary_text)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    summary.append(summary_text)\n",
    "\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb24312-bd40-4279-93a9-9cebacc46da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1). Drug delivery technologies can be classified into three primary categories depending on their physical form during administration: solid, liquid, or gas. Solid-form systems include implants, depots, particles, and capsules. Liquid-form devices contain a controlled-release suspension (CRS) or liposomes. Finally, gas-form technologies rely on propulsion mechanisms such as compressed gases (e.g., CO2), pressurized fluids, or surface tensionbased techniques to transport and deliver drugs in vivo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      77.74 ms /   125 runs   (    0.62 ms per token,  1607.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   38708.79 ms /   125 runs   (  309.67 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   39198.73 ms\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with the text to summarize\n",
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "# Call the language model with the formatted prompt\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=200,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "# Get the generated text from the output\n",
    "generated_text = output['choices'][0]['text']\n",
    "\n",
    "# Find the position of \"Summary:\" in the text\n",
    "summary_index = generated_text.find(\"Summary:\")\n",
    "\n",
    "# Extract everything after \"Summary:\"\n",
    "summary_text = generated_text[summary_index + len(\"Summary:\"):].strip()\n",
    "\n",
    "# Print the extracted summary\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92e87ead-5552-49e6-9c20-728f69e934b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-3464f2b1-a673-4d55-9a81-9382fda885bb', 'object': 'text_completion', 'created': 1701342357, 'model': '../models/llama-2-7b.Q2_K.gguf', 'choices': [{'text': 'Summarize the following text: Medicine relies on the use of pharmacologically active agents (therapeutics or drugs) to manage or reverse the course of disease. The current global pharmaceutical market is valued at $980 billion annually, and, in the U.S., nearly 50% of the population has used at least one prescription medication in the past 30 days. \\nIn the ideal case, drugs would be applied in vivo at exactly the therapeutic concentration and would precisely target cells that cause disease. However, drug delivery is not easily controlled. Drug release rates, cell-and tissue-specific targeting, and drug stability are difficult to predict. To address these limitations, drug delivery systems (DDS) have been designed using a wide array of materials and chemical strategies. Here, we define DDS as technologies that are designed to improve the specificity of therapeutics by stabilizing them in vivo, controlling their release, and localizing their effect. Many materials have released therapeutics for prolonged periods of time and at targeted locations within the body; the properties of DDS are tailored to the physicochemical attributes of the drug and the intended route of administration (Figure\\nSummary: This paper introduces a new paradigm of science and medicine based on holistic human-centered technologies. We review recent developments in targeting cells by size, genetics, or biomarkers and then showcase emerging strategies for localizing therapeutic agents to diseased tissues via nanoscale and molecularly engineered drug delivery systems (DDS). We also discuss the unique properties of DDS that make them critical components in personalized medicine. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 281, 'completion_tokens': 105, 'total_tokens': 386}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      64.93 ms /   105 runs   (    0.62 ms per token,  1617.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   85886.35 ms /   280 tokens (  306.74 ms per token,     3.26 tokens per second)\n",
      "llama_print_timings:        eval time =   40193.95 ms /   104 runs   (  386.48 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =  126543.56 ms\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with the text to summarize\n",
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "# Call the language model with the formatted prompt\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=500,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4709ffa6-a852-49af-a03c-0721357639ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Drug delivery systems (DDS) are designed to control the release rate and specificity of pharmacologically active agents (drugs). DDS may be formulated as suspensions, gels, or particles. The targeted site for drug delivery is often defined by chemical attributes of the therapeutic agent itself. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3448.41 ms\n",
      "llama_print_timings:      sample time =      43.36 ms /    71 runs   (    0.61 ms per token,  1637.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   23284.97 ms /    71 runs   (  327.96 ms per token,     3.05 tokens per second)\n",
      "llama_print_timings:       total time =   23568.61 ms\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = \"Summarize the following text: {}\\nSummary:\".format(paragraphs[0])\n",
    "\n",
    "output = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=200,  # Increased max_tokens\n",
    "    stop=[\"\\n\"],     # Adjusted stop token\n",
    "    echo=False       # Turn off echo\n",
    ")\n",
    "\n",
    "# Extract the generated text from the output\n",
    "generated_text = output['choices'][0]['text']\n",
    "\n",
    "# Print the generated text (which should be the summary)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eac2200a-86a7-4640-bba1-33e7f12ac001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  250.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "llama_new_context_with_model: compute buffer total size = 71.91 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Summarizing paragraphs:   4%|                                        | 1/24 [00:39<15:15, 39.79s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      23.43 ms /    36 runs   (    0.65 ms per token,  1536.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28421.43 ms /   126 tokens (  225.57 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:        eval time =   11233.95 ms /    35 runs   (  320.97 ms per token,     3.12 tokens per second)\n",
      "llama_print_timings:       total time =   39786.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "Summarizing paragraphs:   8%|                                      | 2/24 [01:31<17:14, 47.02s/it]llama_print_timings:      sample time =      48.73 ms /    79 runs   (    0.62 ms per token,  1621.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28400.65 ms /   109 tokens (  260.56 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:        eval time =   23405.02 ms /    78 runs   (  300.06 ms per token,     3.33 tokens per second)\n",
      "llama_print_timings:       total time =   52085.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  12%|                                    | 3/24 [01:41<10:30, 30.03s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       3.44 ms /     6 runs   (    0.57 ms per token,  1745.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8322.58 ms /    36 tokens (  231.18 ms per token,     4.33 tokens per second)\n",
      "llama_print_timings:        eval time =    1454.86 ms /     5 runs   (  290.97 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =    9799.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  17%|                                   | 4/24 [02:57<16:00, 48.03s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      71.61 ms /   100 runs   (    0.72 ms per token,  1396.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33250.07 ms /   121 tokens (  274.79 ms per token,     3.64 tokens per second)\n",
      "llama_print_timings:        eval time =   41911.40 ms /    99 runs   (  423.35 ms per token,     2.36 tokens per second)\n",
      "llama_print_timings:       total time =   75627.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  21%|                                 | 5/24 [03:22<12:32, 39.62s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       0.74 ms /     1 runs   (    0.74 ms per token,  1344.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24690.26 ms /    99 tokens (  249.40 ms per token,     4.01 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   24695.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  25%|                               | 6/24 [04:34<15:13, 50.74s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      68.75 ms /   100 runs   (    0.69 ms per token,  1454.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34343.83 ms /   139 tokens (  247.08 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:        eval time =   37540.59 ms /    99 runs   (  379.20 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =   72322.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  29%|                             | 7/24 [04:59<12:00, 42.37s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      26.48 ms /    39 runs   (    0.68 ms per token,  1472.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13105.67 ms /    52 tokens (  252.03 ms per token,     3.97 tokens per second)\n",
      "llama_print_timings:        eval time =   11879.79 ms /    38 runs   (  312.63 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time =   25139.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  33%|                            | 8/24 [05:57<12:39, 47.49s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      41.29 ms /    62 runs   (    0.67 ms per token,  1501.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31826.11 ms /   114 tokens (  279.18 ms per token,     3.58 tokens per second)\n",
      "llama_print_timings:        eval time =   26302.86 ms /    61 runs   (  431.19 ms per token,     2.32 tokens per second)\n",
      "llama_print_timings:       total time =   58455.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      49.95 ms /    70 runs   (    0.71 ms per token,  1401.54 tokens per second)\n",
      "Summarizing paragraphs:  38%|                          | 9/24 [06:59<12:59, 51.95s/it]llama_print_timings: prompt eval time =   36395.60 ms /   132 tokens (  275.72 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:        eval time =   25051.26 ms /    69 runs   (  363.06 ms per token,     2.75 tokens per second)\n",
      "llama_print_timings:       total time =   61745.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "Summarizing paragraphs:  42%|                        | 10/24 [07:51<12:05, 51.85s/it]llama_print_timings:      sample time =      17.27 ms /    26 runs   (    0.66 ms per token,  1505.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39038.00 ms /   119 tokens (  328.05 ms per token,     3.05 tokens per second)\n",
      "llama_print_timings:        eval time =   12420.21 ms /    25 runs   (  496.81 ms per token,     2.01 tokens per second)\n",
      "llama_print_timings:       total time =   51628.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  46%|                      | 11/24 [08:50<11:42, 54.03s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      42.47 ms /    63 runs   (    0.67 ms per token,  1483.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36978.96 ms /   110 tokens (  336.17 ms per token,     2.97 tokens per second)\n",
      "llama_print_timings:        eval time =   21734.27 ms /    62 runs   (  350.55 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:       total time =   58980.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  50%|                    | 12/24 [10:05<12:07, 60.58s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      66.29 ms /   100 runs   (    0.66 ms per token,  1508.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38499.90 ms /   124 tokens (  310.48 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:        eval time =   36612.19 ms /    99 runs   (  369.82 ms per token,     2.70 tokens per second)\n",
      "llama_print_timings:       total time =   75558.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  54%|                  | 13/24 [10:33<09:15, 50.52s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     1 runs   (    0.91 ms per token,  1101.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27351.24 ms /    64 tokens (  427.36 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   27360.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "Summarizing paragraphs:  58%|                 | 14/24 [11:18<08:09, 48.92s/it]llama_print_timings:      sample time =       8.29 ms /    12 runs   (    0.69 ms per token,  1448.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41021.34 ms /   119 tokens (  344.72 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4149.11 ms /    11 runs   (  377.19 ms per token,     2.65 tokens per second)\n",
      "llama_print_timings:       total time =   45221.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  62%|               | 15/24 [11:59<06:59, 46.63s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      52.94 ms /    73 runs   (    0.73 ms per token,  1379.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14187.34 ms /    52 tokens (  272.83 ms per token,     3.67 tokens per second)\n",
      "llama_print_timings:        eval time =   26779.28 ms /    72 runs   (  371.93 ms per token,     2.69 tokens per second)\n",
      "llama_print_timings:       total time =   41308.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  67%|             | 16/24 [12:44<06:08, 46.03s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      15.06 ms /    20 runs   (    0.75 ms per token,  1328.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37135.84 ms /   131 tokens (  283.48 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:        eval time =    7418.40 ms /    19 runs   (  390.44 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:       total time =   44644.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      64.04 ms /    91 runs   (    0.70 ms per token,  1421.01 tokens per second)\n",
      "Summarizing paragraphs:  71%|            | 17/24 [13:42<05:47, 49.59s/it]llama_print_timings: prompt eval time =   15301.98 ms /    58 tokens (  263.83 ms per token,     3.79 tokens per second)\n",
      "llama_print_timings:        eval time =   42061.07 ms /    90 runs   (  467.35 ms per token,     2.14 tokens per second)\n",
      "llama_print_timings:       total time =   57849.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  75%|          | 18/24 [14:42<05:17, 52.91s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      22.74 ms /    31 runs   (    0.73 ms per token,  1363.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47102.65 ms /   108 tokens (  436.14 ms per token,     2.29 tokens per second)\n",
      "llama_print_timings:        eval time =   13400.40 ms /    30 runs   (  446.68 ms per token,     2.24 tokens per second)\n",
      "llama_print_timings:       total time =   60659.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      64.74 ms /   100 runs   (    0.65 ms per token,  1544.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26659.07 ms /   103 tokens (  258.83 ms per token,     3.86 tokens per second)\n",
      "llama_print_timings:        eval time =   40117.10 ms /    99 runs   (  405.22 ms per token,     2.47 tokens per second)\n",
      "llama_print_timings:       total time =   67243.26 ms\n",
      "Summarizing paragraphs:  79%|        | 19/24 [15:50<04:46, 57.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  83%|      | 20/24 [16:42<03:43, 55.75s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      24.40 ms /    33 runs   (    0.74 ms per token,  1352.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37978.42 ms /   115 tokens (  330.25 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:        eval time =   14185.47 ms /    32 runs   (  443.30 ms per token,     2.26 tokens per second)\n",
      "llama_print_timings:       total time =   52335.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs:  88%|     | 21/24 [18:05<03:11, 63.80s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =      75.90 ms /   100 runs   (    0.76 ms per token,  1317.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36103.20 ms /   122 tokens (  295.93 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:        eval time =   45934.08 ms /    99 runs   (  463.98 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time =   82565.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  92%|   | 22/24 [18:41<01:50, 55.49s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       3.42 ms /     5 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34308.17 ms /   113 tokens (  303.61 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1761.40 ms /     4 runs   (  440.35 ms per token,     2.27 tokens per second)\n",
      "llama_print_timings:       total time =   36099.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "Summarizing paragraphs:  96%| | 23/24 [18:51<00:41, 41.83s/it]\n",
      "llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       0.85 ms /     1 runs   (    0.85 ms per token,  1172.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9964.97 ms /    35 tokens (  284.71 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    9974.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Summarizing paragraphs: 100%|| 24/24 [19:16<00:00, 36.86s/it]llama_print_timings:        load time =   28421.60 ms\n",
      "llama_print_timings:      sample time =       0.62 ms /     1 runs   (    0.62 ms per token,  1618.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25246.29 ms /    65 tokens (  388.40 ms per token,     2.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   25250.28 ms\n",
      "Summarizing paragraphs: 100%|| 24/24 [19:16<00:00, 48.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollable because it occurs in the human body, which contains a multitude of cell types with different functions that are interconnected by complex, bidirectional communication pathways. with targeted deposition in specific tissues or cells. In addition, some materials have been designed to release therapeutics only when they are exposed to a pH below 4.6 (pH 1), which occurs at the tumor site. Although there is great promise for these technologies, their development requires a high degree of precision and attention to detail. 1-2). - ditions under which CRS perform well (Table I). First, the drug release profile should be consistent with a therapeutic window; that is, the rate of delivery should not exceed the metabolism/elimination rate. Second, a high molecular weight material should protect the drug from degradation and enzymatic attack to maintain stability while in vivo. Third, CRS should be fabricated in a way to make them easy to load with drugs untion, and a variety of metabolic conditions (5). It was therefore logical that the first siRNAs targeted to the liver were those that blocked protein expression in order to treat these diseases. However, it wasnt long before siRNA delivery to the liver was recognized as an important issue because of its rapid clearance from this organ (6). The first siRNA delivery method used the cationic polymer PEI to conjugate si They may also have utility in other areas, such as as cancer therapy. However, the first practical applications of mRNA-based drugs will not become available for several years. bofructose depots into subcutaneous tissue in a rodent model has been reported to result in 35 days of drug release at the desired site of action (the blood-brain barrier, BBB), with no discernible systemic exposure [4]. strategies have been particularly effective in solid tumors that can be surgically accessed and treated with locally delivered DDS. For example, using cytarabine DDS implanted at the site of brain tumor resection has demonstrated efficacy and reduced side effects. However, more widely applicable cancer-selective approaches remain to be developed. led release, a coated catheter is needed to perform stent-assisted drug delivery (SADD). by converting them from a fluid state into a solid state. This technique is most commonly employed for soft-tissue augmentation with collagen hydrogels that are injected at room temperature and then triggered to transition to a solid state upon application of heat or energy (e.g., microwaves). ydrolyzing poly(methacrylic acid) copolymers can be used to form injectable depots that remain in situ for months or years. For example, hydrophilic poly(ethyl acetate methacrylate) depots formulated with cytosine arabinoside (Ara-C) precipitate at 20C and the polymer is reversibly swollen to solubility in aqueous solution ng injectable that has several potential applications including: In contrast, subcutaneous injection is convenient and safe but limited by its relatively long residence time (~4 d). Therefore, we designed a sustained-release formulation of GLP analogues that can be administered once every week via subcutaneous injection, to improve compliance and minimize the frequency of visits to clinics.  tion  and modulate immune responses (Figure 14.3). We have now shown that the major obstacle to such therapy is not a lack of donor cells but rather their inability to survive in vivo after transplantation into immuno-suppressed hosts (1). To determine whether this is true for other human diseases, we examined whether cultured macrophages from patients with familial giant cell arteritis were also unable to survive in recipient animals. ancements in formulation technologies to provide sustained drug release, controlled drug availability, and spatial control of the drug at the injection site. ipped the scales for many drugs that were previously unavailable due to their poor pharmacokinetic profiles (Baek, 2013). These biodegradable polymers are now being used as vehicles for hydrophobic actives in a range of chronic diseases such as cancer and inflammatory disease. Increasingly these polymers will become the default drug delivery system to treat chronic disease due to their high loading capacity, controlled release -risk the clinician by enabling a predictable, repeatable dose form that is controlled for duration in order to improve efficacy outcomes. 1) active ingredients across biological barriers; 2) to act as targeting moieties, and 3) to serve as vehicles for cellular encapsulation (Figure 4). In the end, we envision that the next generation of drug carriers will be comprised of a combination of existing materials. These will likely include hydrogels, liposomes, virions, micelles, nanospheres and nanoparticles (e. the nucleus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_l = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "              n_ctx=500, \n",
    "              n_gpu_layers=-1)\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_l(\n",
    "        formatted_prompt,\n",
    "        max_tokens=100,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220a7d3-4ff7-497e-9a62-f808fb53b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_l = Llama(model_path=\"../models/llama-2-7b.Q2_K.gguf\",\n",
    "              n_ctx=500, \n",
    "              n_gpu_layers=-1)\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    # Tokenize the individual paragraph to check its length\n",
    "    tokens = tokenizer.tokenize(paragraph)\n",
    "    \n",
    "    # Skip paragraphs that are too long\n",
    "    if len(tokens) > 512:\n",
    "        continue\n",
    "\n",
    "    # Simplified prompt\n",
    "    formatted_prompt = \"Summarize this paragraph: {}\".format(paragraph)\n",
    "    \n",
    "    # Generate summary with a possibly increased max_tokens\n",
    "    output = llm_l(\n",
    "        formatted_prompt,\n",
    "        max_tokens=60,  # Increased max_tokens\n",
    "        stop=[\"\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Extracting summary text\n",
    "    generated_text = output['choices'][0]['text'].strip()\n",
    "    \n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149abdb3-6a8e-4df2-ba43-d14083935a73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# llama-2-7b-chat.ggmlv3.q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dedc3-0367-429e-92fe-2b1cc58a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your local model directory\n",
    "local_model_directory = \"../models/llama-2-7b-chat.ggmlv3.q8_0\"  # Adjust this path\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_directory)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4032cb-b67c-4a2e-b166-86095a0444e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline\n",
    "prompt = f\"Summarize this: {}\".format(combined_text)\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Summary:\\n\"\n",
    "pipe = pipeline(task=\"summarization\", # \"text-generation\" \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_length=200)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a6b78-d651-4ff3-ba0c-e61e0b6c0ba7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NousResearch/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb0ca7-b2c4-40c3-836a-429e6ab9ad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8119a36f661f4344ab8c9f87bc245771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa808954d6fe46b2bc58e65d291c937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384c1206c84b461e91d14f1be01aff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c15376e0ad45efab76608e7e82cb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddc70837d4242be94633f8c57a47711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b01ac-72b9-4dee-8ee5-b08f350b17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Run text generation pipeline\n",
    "prompt = f\"Summarize this: {combined_text}\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Summary:\\n\"\n",
    "\n",
    "pipe = pipeline(task=\"summarization\", # \"text-generation\" \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59b36b9-2e4a-448e-9648-5634c2fa35ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>input</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRACT\\n Medicine relies on the use of pharm...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>Medicine relies on the use of pharmacologicall...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR CONTROLLED</td>\n",
       "      <td>RELEASE One important class of DDS is controll...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>SYSTEMIC RNA DELIVERY</td>\n",
       "      <td>RNAs can manipulate gene expression through se...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TITLE_PARAGRAPH</td>\n",
       "      <td>DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY</td>\n",
       "      <td>One potential limitation to systemic administr...</td>\n",
       "      <td>Emerging Frontiers in Drug Delivery.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                        title  \\\n",
       "0         ABSTRACT                                     ABSTRACT   \n",
       "1  TITLE_PARAGRAPH                                 INTRODUCTION   \n",
       "2  TITLE_PARAGRAPH         DRUG DELIVERY SYSTEMS FOR CONTROLLED   \n",
       "3  TITLE_PARAGRAPH                        SYSTEMIC RNA DELIVERY   \n",
       "4  TITLE_PARAGRAPH  DRUG DELIVERY SYSTEMS FOR LOCALIZED THERAPY   \n",
       "\n",
       "                                               input  \\\n",
       "0  ABSTRACT\\n Medicine relies on the use of pharm...   \n",
       "1  Medicine relies on the use of pharmacologicall...   \n",
       "2  RELEASE One important class of DDS is controll...   \n",
       "3  RNAs can manipulate gene expression through se...   \n",
       "4  One potential limitation to systemic administr...   \n",
       "\n",
       "                               data_source  \n",
       "0  Emerging Frontiers in Drug Delivery.txt  \n",
       "1  Emerging Frontiers in Drug Delivery.txt  \n",
       "2  Emerging Frontiers in Drug Delivery.txt  \n",
       "3  Emerging Frontiers in Drug Delivery.txt  \n",
       "4  Emerging Frontiers in Drug Delivery.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5396c49-f263-468c-b191-615e89558675",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Llama2 Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a89815b-6c48-4d1b-82ac-a75c1ddd63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your model directory\n",
    "model_directory = \"../models/Llama2-7b-Summarizer\" \n",
    "\n",
    "def summarize(input_text, model_directory):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "    \n",
    "    # Prepare your input text\n",
    "    input_text = input_text\n",
    "    input_ids = tokenizer.encode(pdf, \n",
    "                                 return_tensors='pt')\n",
    "    \n",
    "    # Set the desired maximum length for the generated text\n",
    "    max_length = 200\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(input_ids, max_length=max_length)\n",
    "    \n",
    "    # Decode the output to text\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "\n",
    "    return decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d92cd0-b72c-4e46-9b68-70059c54f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing paragraphs:   0%|                                                  | 0/24 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "model_directory = \"../models/Llama2-7b-Summarizer\" \n",
    "\n",
    "summary = []\n",
    "for paragraph in tqdm(processed_paragraphs, desc=\"Summarizing paragraphs\"):\n",
    "    generated_text = summarize(paragraph, model_directory)\n",
    "    # Append only if generated_text is not empty\n",
    "    if generated_text:\n",
    "        summary.append(generated_text)\n",
    "# Concatenating with space and ensuring each summary starts on a new line\n",
    "combined_summary = ' '.join(summary)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead4d35-a2fc-4d86-9934-93b15281a06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aef14f6c9a6412eaacec74534af65a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_974883c2255b4908a8b66646af6e2067",
       "IPY_MODEL_fc9d9a5ba2844a2e9d9040afc092f598",
       "IPY_MODEL_49bc2f1c67cf419581d1082f34a02fb9"
      ],
      "layout": "IPY_MODEL_c3e1f3541eef486881a3b60ed5be4965"
     }
    },
    "0f417848bd944590901805301d49054a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15a2b12901c246d882fd798609fceb69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d234f78da5f456e8fe4d85a8e129f9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25b50cb90d7e463ba8736ecfcf39626d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a982474fc59439c813fcb916599126f",
      "value": 7
     }
    },
    "2246a631405e4240a672208743eb1333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25b50cb90d7e463ba8736ecfcf39626d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fa5357ba5f847139d37efe457a93285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_346e498e9a474aceb4178797b0329929",
       "IPY_MODEL_1d234f78da5f456e8fe4d85a8e129f9d",
       "IPY_MODEL_b7e7ef816db44dafb4bf3bfeaa1a09c2"
      ],
      "layout": "IPY_MODEL_70eb92241b164feb83a373daa57118fe"
     }
    },
    "3276e963687c4e12b8c0f14a1535f014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2e304a7f220407bada34a20ac415424",
       "IPY_MODEL_f0a292d34c984da19bad182338fca1f9",
       "IPY_MODEL_4de16a026e9b46feb421bd269a681098"
      ],
      "layout": "IPY_MODEL_2246a631405e4240a672208743eb1333"
     }
    },
    "346e498e9a474aceb4178797b0329929": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9126d9ddf8524907b3825f6c348fb0d6",
      "placeholder": "",
      "style": "IPY_MODEL_dd3cf04d4590446a9478427c4359eed5",
      "value": "Batches: 100%"
     }
    },
    "359a097446484f6b9f0a408b9a36e86d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49bc2f1c67cf419581d1082f34a02fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68a743a2ba724a38b9677dd467d1bf36",
      "placeholder": "",
      "style": "IPY_MODEL_9c0fc847dda64fc1addeb687298a04f1",
      "value": " 7/7 [00:00&lt;00:00,  9.23it/s]"
     }
    },
    "4d004a9328a44af2afa5a1ea017ac5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de16a026e9b46feb421bd269a681098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68cabc74f8fc4dce9d274d0133676324",
      "placeholder": "",
      "style": "IPY_MODEL_f53d67feef24424ea9eb56615ab58dcc",
      "value": " 299/299 [00:00&lt;00:00, 12975.92it/s]"
     }
    },
    "4e3dd8a6d46c4603b894e2c72aeefa18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e9b0386b84042649fafb3c1fc9eb2e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5786899f30e64e819074b00c3ee75103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa40d31a5c0d42ef93500c7134e9c4de",
       "IPY_MODEL_c3c244d98b084208ab61b181d18b7297",
       "IPY_MODEL_b28f8f1966014e8bb8969e542cfab395"
      ],
      "layout": "IPY_MODEL_359a097446484f6b9f0a408b9a36e86d"
     }
    },
    "58e4d3b1b6c14208b759f2c320a76f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a72a1b09ac14fd5bdcc97771494fdc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a982474fc59439c813fcb916599126f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "657e37ed3def4aef8b81544d797b5934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68a743a2ba724a38b9677dd467d1bf36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68cabc74f8fc4dce9d274d0133676324": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70eb92241b164feb83a373daa57118fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9126d9ddf8524907b3825f6c348fb0d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "974883c2255b4908a8b66646af6e2067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebf0f773fcbc4aa7ad597cdf8873a0fa",
      "placeholder": "",
      "style": "IPY_MODEL_adf903e9a4d1435e8bf1c38cab061326",
      "value": "Batches: 100%"
     }
    },
    "9c0fc847dda64fc1addeb687298a04f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa40d31a5c0d42ef93500c7134e9c4de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f417848bd944590901805301d49054a",
      "placeholder": "",
      "style": "IPY_MODEL_58e4d3b1b6c14208b759f2c320a76f39",
      "value": "Batches: 100%"
     }
    },
    "adf903e9a4d1435e8bf1c38cab061326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b28f8f1966014e8bb8969e542cfab395": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15a2b12901c246d882fd798609fceb69",
      "placeholder": "",
      "style": "IPY_MODEL_5a72a1b09ac14fd5bdcc97771494fdc0",
      "value": " 10/10 [00:01&lt;00:00, 10.63it/s]"
     }
    },
    "b7e7ef816db44dafb4bf3bfeaa1a09c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d701cb9f7e3b414ba22c4960193018af",
      "placeholder": "",
      "style": "IPY_MODEL_deba6a162cb645f497f38392f032f03a",
      "value": " 7/7 [00:00&lt;00:00,  9.32it/s]"
     }
    },
    "bc7b487ea278491ca16a1032d9ba5863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bcef0fa46e044c27ab0826adf73a60fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3c244d98b084208ab61b181d18b7297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffc7b241a64d4cec96db26f507912b72",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e3dd8a6d46c4603b894e2c72aeefa18",
      "value": 10
     }
    },
    "c3e1f3541eef486881a3b60ed5be4965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d701cb9f7e3b414ba22c4960193018af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3cf04d4590446a9478427c4359eed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "deba6a162cb645f497f38392f032f03a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8337d6b59004e0eb4f12f26ab28b546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf0f773fcbc4aa7ad597cdf8873a0fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0a292d34c984da19bad182338fca1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_657e37ed3def4aef8b81544d797b5934",
      "max": 299,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc7b487ea278491ca16a1032d9ba5863",
      "value": 299
     }
    },
    "f2e304a7f220407bada34a20ac415424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcef0fa46e044c27ab0826adf73a60fc",
      "placeholder": "",
      "style": "IPY_MODEL_e8337d6b59004e0eb4f12f26ab28b546",
      "value": "Filtering: 100%"
     }
    },
    "f53d67feef24424ea9eb56615ab58dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc9d9a5ba2844a2e9d9040afc092f598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d004a9328a44af2afa5a1ea017ac5cf",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e9b0386b84042649fafb3c1fc9eb2e2",
      "value": 7
     }
    },
    "ffc7b241a64d4cec96db26f507912b72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
